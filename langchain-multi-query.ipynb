{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2-XDGL6Oi6h4"
   },
   "source": [
    "# LangChain Multi-Query for RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "One of the advanced features in LangChain is Multi-Query for RAG, which allows for more efficient and effective information retrieval from large datasets.\n",
    "\n",
    "## What is Multi-Query for RAG?\n",
    "\n",
    "Multi-Query for RAG is a technique used to enhance the retrieval process in retrieval-augmented generation systems. It involves generating multiple queries from a single input and using these queries to retrieve a more diverse and relevant set of documents from the database. This approach improves the quality of the retrieved information, leading to better and more accurate generated responses.\n",
    "\n",
    "## Key Components\n",
    "\n",
    "1. **Query Generation**: Instead of generating a single query, the system creates multiple queries based on different aspects or perspectives of the input. These queries can be variations or expansions of the original input.\n",
    "\n",
    "2. **Document Retrieval**: Each generated query is used to search the indexed documents. The results from all queries are combined to form a more comprehensive set of retrieved documents.\n",
    "\n",
    "3. **Response Generation**: The retrieved documents are then used to generate the final response. By incorporating information from multiple queries, the generated response is more likely to be accurate and relevant.\n",
    "\n",
    "## Benefits of Multi-Query for RAG\n",
    "\n",
    "- **Improved Diversity**: Multiple queries increase the chances of retrieving diverse pieces of information, covering different aspects of the input topic.\n",
    "- **Enhanced Relevance**: Combining results from multiple queries can lead to a more relevant and contextually appropriate set of documents.\n",
    "- **Robustness**: Multi-query retrieval is more robust to variations and ambiguities in the input, as different queries can capture different interpretations.\n",
    "\n",
    "## Example Workflow\n",
    "\n",
    "1. **Input Processing**: The user provides an input query.\n",
    "2. **Query Generation**: The system generates multiple queries from the input.\n",
    "3. **Document Retrieval**: Each query is used to search the document index, and the results are combined.\n",
    "4. **Response Generation**: The retrieved documents are used by the language model to generate the final response.\n",
    "\n",
    "<div>\n",
    "<img src=\"https://education-team-2020.s3.eu-west-1.amazonaws.com/ai-eng/images-langchain-4/muti.avif\" alt='auto' width=\"1000\"/>\n",
    "</div>\n",
    "\n",
    "## Example Code\n",
    "\n",
    "Here is a simplified example of how Multi-Query for RAG might be implemented using LangChain:\n",
    "\n",
    "```python\n",
    "from langchain.chains import MultiQueryRAGChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.indexes import VectorIndex\n",
    "\n",
    "# Define the base prompt template\n",
    "base_prompt_template = PromptTemplate(\n",
    "    input_variables=[\"input_text\"],\n",
    "    template=\"Generate multiple queries from the following input: {input_text}\"\n",
    ")\n",
    "\n",
    "# Initialize the language model\n",
    "llm = OpenAI(model=\"text-davinci-003\")\n",
    "\n",
    "# Create the Multi-Query RAG Chain\n",
    "multi_query_chain = MultiQueryRAGChain(\n",
    "    prompt=base_prompt_template,\n",
    "    llm=llm,\n",
    "    index=VectorIndex()  # Assuming VectorIndex is already populated with documents\n",
    ")\n",
    "\n",
    "# Input text\n",
    "input_text = \"Tell me about the history of artificial intelligence.\"\n",
    "\n",
    "# Run the chain and get the response\n",
    "response = multi_query_chain.run({\"input_text\": input_text})\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CPmfrdJ9_2YA"
   },
   "source": [
    "## Getting Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S4Py-rVqx-I0"
   },
   "source": [
    "We will download an existing dataset from Hugging Face Datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iatOGmKgz8NE",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "data = load_dataset(\"data\", split=\"train\")\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load the GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Add a padding token\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "def read_files_in_directory(directory):\n",
    "    data = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "                    data.append({'file_name': file, 'content': content})\n",
    "    return data\n",
    "\n",
    "def summarize_text(text):\n",
    "    # Preprocess the text for GPT-2\n",
    "    input_text = \"summarize: \" + text\n",
    "    inputs = tokenizer.encode_plus(input_text, return_tensors=\"pt\", max_length=512, truncation=True, padding='max_length')\n",
    "    \n",
    "    # Get input_ids and attention_mask\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "    \n",
    "    # Generate the summary\n",
    "    summary_ids = model.generate(\n",
    "        input_ids, \n",
    "        attention_mask=attention_mask, \n",
    "        max_length=150, \n",
    "        min_length=40, \n",
    "        length_penalty=2.0, \n",
    "        num_beams=4, \n",
    "        early_stopping=True, \n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return summary\n",
    "\n",
    "def extract_data(data):\n",
    "    summaries = []\n",
    "    for item in data:\n",
    "        summary = summarize_text(item['content'])\n",
    "        summaries.append({'file_name': item['file_name'], 'summary': summary})\n",
    "    return summaries\n",
    "\n",
    "base_directory = 'data'\n",
    "directories = ['landmarks', 'municipalities', 'news/elmundo_chunked_en_page1_15years', 'news/elmundo_chunked_es_page1_15years', 'news/elmundo_chunked_es_page1_40years']\n",
    "\n",
    "data_repository = {}\n",
    "\n",
    "# Process each directory\n",
    "for dir_name in directories:\n",
    "    full_path = os.path.join(base_directory, dir_name)\n",
    "    dir_data = read_files_in_directory(full_path)\n",
    "  \n",
    "\n",
    "# Save the data repository to a JSON file\n",
    "with open('puerto_rico_data_repository.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(data_repository, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print('Data repository created successfully.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load the GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Add a padding token\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "def read_files_in_directory(directory):\n",
    "    data = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "                    data.append({'file_name': file, 'content': content})\n",
    "    return data\n",
    "\n",
    "def summarize_text(text):\n",
    "    # Preprocess the text for GPT-2\n",
    "    input_text = \"summarize: \" + text\n",
    "    inputs = tokenizer.encode_plus(input_text, return_tensors=\"pt\", max_length=512, truncation=True, padding='max_length')\n",
    "    \n",
    "    # Get input_ids and attention_mask\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "    \n",
    "    # Generate the summary\n",
    "    summary_ids = model.generate(\n",
    "        input_ids, \n",
    "        attention_mask=attention_mask, \n",
    "        max_length=150, \n",
    "        min_length=40, \n",
    "        length_penalty=2.0, \n",
    "        num_beams=4, \n",
    "        early_stopping=True, \n",
    "        pad_token_id=tokenizer.pad_token_id\n",
    "    )\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return summary\n",
    "\n",
    "def extract_data(data):\n",
    "    summaries = []\n",
    "    for item in data:\n",
    "        summary = summarize_text(item['content'])\n",
    "        summaries.append({'file_name': item['file_name'], 'summary': summary})\n",
    "    return summaries\n",
    "\n",
    "base_directory = 'data'\n",
    "directories = ['landmarks', 'municipalities', 'news/elmundo_chunked_en_page1_15years', 'news/elmundo_chunked_es_page1_15years', 'news/elmundo_chunked_es_page1_40years']\n",
    "\n",
    "data_repository = {}\n",
    "\n",
    "# Process each directory\n",
    "for dir_name in directories:\n",
    "    full_path = os.path.join(base_directory, dir_name + '_small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P7E6JYtb0cW7",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "\n",
    "docs = []\n",
    "\n",
    "for row in data:\n",
    "    doc = Document(\n",
    "        page_content=row[\"text\"],\n",
    "        metadata={\n",
    "            \"title\": row[\"title\"],\n",
    "            \"source\": row[\"source\"],\n",
    "            \"id\": row[\"id\"],\n",
    "            \"chunk-id\": row[\"chunk-id\"],\n",
    "            \"text\": row[\"chunk\"]\n",
    "        }\n",
    "    )\n",
    "    docs.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = []\n",
    "\n",
    "for row in data:\n",
    "    title = row.get(\"title\", \"No Title\")  # Provide a default value if 'title' key is missing\n",
    "    source = row.get(\"source\", \"No Source\")\n",
    "    doc_id = row.get(\"id\", \"No ID\")\n",
    "    chunk_id = row.get(\"chunk-id\", \"No Chunk ID\")\n",
    "    text = row.get(\"text\", \"No Text\")\n",
    "\n",
    "    doc = Document(\n",
    "        page_content=text,\n",
    "        metadata={\n",
    "            \"title\": title,\n",
    "            \"source\": source,\n",
    "            \"id\": doc_id,\n",
    "            \"chunk-id\": chunk_id,\n",
    "            \"text\": text\n",
    "        }\n",
    "    )\n",
    "    docs.append(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, find_dotenv\n",
    "import os\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "OPENAI_API_KEY  = os.getenv('OPENAI_API_KEY')\n",
    "HUGGINGFACEHUB_API_TOKEN = os.getenv('HUGGINGFACEHUB_API_TOKEN')\n",
    "\n",
    "# initialize connection to pinecone (get API key at app.pinecone.io)\n",
    "PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\") or \"YOUR_API_KEY\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yb540kEs_6PZ"
   },
   "source": [
    "## Embedding and Vector DB Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BlKEmBZMBxtd"
   },
   "source": [
    "Initialize our embedding model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qZ6vTiDPBznz"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "model_name = \"text-embedding-ada-002\"\n",
    "\n",
    "# Get the OpenAI API key\n",
    "OPENAI_API_KEY = getpass(\"Enter your OpenAI API key: \")\n",
    "# Initialize the OpenAI embeddings\n",
    "\n",
    "embed = OpenAIEmbeddings(\n",
    "    model=model_name, openai_api_key=OPENAI_API_KEY, disallowed_special=()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IurEkeeI-IYl"
   },
   "source": [
    "Now we create our vector DB to store our vectors. For this we need to get a [free Pinecone API key](https://app.pinecone.io) — the API key can be found in the \"API Keys\" button found in the left navbar of the Pinecone dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "# configure client\n",
    "pc = Pinecone(api_key=\"pcsk_2rpXkM_9sCN5zHBLDDw7mBs4LAWwfaWdtRnJSeQ7Szvjij5YeVyjHKw2okL4pogSrt7u7C\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we setup our index specification, this allows us to define the cloud provider and region where we want to deploy our index. You can find a list of all [available providers and regions here](https://docs.pinecone.io/docs/projects)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pinecone import ServerlessSpec\n",
    "\n",
    "spec = ServerlessSpec(\n",
    "    cloud=\"aws\", region=\"us-east-1\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating an index, we set `dimension` equal to to dimensionality of Ada-002 (`1536`), and use a `metric` also compatible with Ada-002 (this can be either `cosine` or `dotproduct`). We also pass our `spec` to index initialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nL3KFF9E9Qb_"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "index_name = \"langchain-multi-query-demo\"\n",
    "existing_indexes = [\n",
    "    index_info[\"name\"] for index_info in pc.list_indexes()\n",
    "]\n",
    "\n",
    "# check if index already exists (it shouldn't if this is first time)\n",
    "if index_name not in existing_indexes:\n",
    "    # if does not exist, create index\n",
    "    pc.create_index(\n",
    "        index_name,\n",
    "        dimension=1536,  # dimensionality of ada 002\n",
    "        metric='dotproduct',\n",
    "        spec=spec\n",
    "    )\n",
    "    # wait for index to be initialized\n",
    "    while not pc.describe_index(index_name).status['ready']:\n",
    "        time.sleep(1)\n",
    "\n",
    "# connect to index\n",
    "index = pc.Index(index_name)\n",
    "time.sleep(1)\n",
    "# view index stats\n",
    "index.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A3B7dHsd6QcP"
   },
   "source": [
    "Populate our index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B7Yi2YGBpTWf"
   },
   "outputs": [],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "thfCYHuSpW4H"
   },
   "outputs": [],
   "source": [
    "# if you want to speed things up to follow along\n",
    "#docs = docs[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pinecone --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "from pinecone import Pinecone, IndexSpec\n",
    "\n",
    "# Prompt for the Pinecone API key securely\n",
    "api_key = getpass.getpass(\"Enter your Pinecone API key: \")\n",
    "\n",
    "# Use the Pinecone API key\n",
    "pc = Pinecone(\n",
    "    api_key=api_key\n",
    ")\n",
    "\n",
    "# Now do stuff\n",
    "if 'my_index' not in pc.list_indexes().names():\n",
    "    spec = IndexSpec(\n",
    "        dimension=128,\n",
    "        metric='cosine'  # Change to your desired metric\n",
    "    )\n",
    "    pc.create_index(name='my_index', spec=spec)\n",
    "\n",
    "# Perform other operations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from pinecone import Index, init\n",
    "\n",
    "# Initialize Pinecone\n",
    "init(api_key=\"your_pinecone_api_key\")\n",
    "index = Index(\"your_index_name\")\n",
    "\n",
    "# Load the GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "def read_files_in_directory(directory):\n",
    "    data = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "                    data.append({'file_name': file, 'content': content})\n",
    "    return data\n",
    "\n",
    "def summarize_text(text):\n",
    "    # Preprocess the text for GPT-2\n",
    "    input_text = \"summarize: \" + text\n",
    "    inputs = tokenizer.encode_plus(input_text, return_tensors=\"pt\", max_length=512, truncation=True, padding='max_length')\n",
    "    \n",
    "    # Get input_ids and attention_mask\n",
    "    input_ids = inputs[\"input_ids\"]\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "    \n",
    "    # Generate the summary\n",
    "    summary_ids = model.generate(\n",
    "        input_ids, \n",
    "        attention_mask=attention_mask, \n",
    "        max_length=150, \n",
    "        min_length=40, \n",
    "        length_penalty=2.0, \n",
    "        num_beams=4, \n",
    "        early_stopping=True, \n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return summary\n",
    "\n",
    "def extract_data(data):\n",
    "    summaries = []\n",
    "    for item in data:\n",
    "        summary = summarize_text(item['content'])\n",
    "        summaries.append({'file_name': item['file_name'], 'summary': summary})\n",
    "    return summaries\n",
    "\n",
    "def trim_metadata(metadata, max_size=40960):\n",
    "    \"\"\"Trim metadata to ensure it fits within the specified max size.\"\"\"\n",
    "    metadata_str = json.dumps(metadata)\n",
    "    if len(metadata_str) <= max_size:\n",
    "        return metadata\n",
    "    # Trim the metadata if it exceeds the max size\n",
    "    trimmed_metadata = {}\n",
    "    for key, value in metadata.items():\n",
    "        if len(json.dumps(trimmed_metadata)) + len(json.dumps({key: value})) > max_size:\n",
    "            break\n",
    "        trimmed_metadata[key] = value\n",
    "    return trimmed_metadata\n",
    "\n",
    "base_directory = 'data'\n",
    "directories = ['landmarks', 'municipalities', 'news/elmundo_chunked_en_page1_15years', 'news/elmundo_chunked_es_page1_15years', 'news/elmundo_chunked_es_page1_40years']\n",
    "\n",
    "data_repository = {}\n",
    "\n",
    "# Process each directory\n",
    "for dir_name in directories:\n",
    "    full_path = os.path.join(base_directory, dir_name)\n",
    "    dir_data = read_files_in_directory(full_path)\n",
    "    summaries = extract_data(dir_data)\n",
    "    data_repository[dir_name] = summaries\n",
    "\n",
    "# Save the data repository to a JSON file\n",
    "with open('puerto_rico_data_repository.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(data_repository, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Upsert data to Pinecone\n",
    "for dir_name, summaries in data_repository.items():\n",
    "    ids = [summary['file_name'] for summary in summaries]\n",
    "    embeds = [embed(summary['summary']) for summary in summaries]  # Replace with your embedding function\n",
    "    metadata = [trim_metadata(summary) for summary in summaries]\n",
    "    to_upsert = zip(ids, embeds, metadata)\n",
    "    index.upsert(vectors=to_upsert)\n",
    "\n",
    "print('Data repository created and upserted to Pinecone successfully.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HXVVU97C6SwT"
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "batch_size = 100\n",
    "\n",
    "for i in tqdm(range(0, len(docs), batch_size)):\n",
    "    i_end = min(len(docs), i+batch_size)\n",
    "    docs_batch = docs[i:i_end]\n",
    "    # get IDs\n",
    "    ids = [f\"{doc.metadata['id']}-{doc.metadata['chunk-id']}\" for doc in docs_batch]\n",
    "    # get text and embed\n",
    "    texts = [d.page_content for d in docs_batch]\n",
    "    embeds = embed.embed_documents(texts=texts)\n",
    "    # get metadata\n",
    "    metadata = [d.metadata for d in docs_batch]\n",
    "    to_upsert = zip(ids, embeds, metadata)\n",
    "    index.upsert(vectors=to_upsert)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8FbngTBzAAU-"
   },
   "source": [
    "## Multi-Query with LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YVVYr13n_Ot2"
   },
   "source": [
    "Now we switch across to using our populated index as a vectorstore in Langchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0ETs0emsAh-K",
    "outputId": "0b1de24b-2f9f-48a6-d8ca-bd3d6aa007e1"
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "text_field = \"text\"\n",
    "\n",
    "vectorstore = Pinecone(index, embed.embed_query, text_field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nW_GCB6a3_N_"
   },
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1iptBAriANrD"
   },
   "source": [
    "We initialize the `MultiQueryRetriever`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yYjztBp2ANHC"
   },
   "outputs": [],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=vectorstore.as_retriever(), llm=llm\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H8qZCd1TAMAn"
   },
   "source": [
    "We set logging so that we can see the queries as they're generated by our LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rgV1eYU6FgX7"
   },
   "outputs": [],
   "source": [
    "# Set logging for the queries\n",
    "import logging\n",
    "\n",
    "logging.basicConfig()\n",
    "logging.getLogger(\"langchain.retrievers.multi_query\").setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jrjwkpJWAaAn"
   },
   "source": [
    "To query with our multi-query retriever we call the `get_relevant_documents` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_DJ4cSJXFinV",
    "outputId": "265900d1-6aa7-4d28-cbbe-e2e95b7df7b4"
   },
   "outputs": [],
   "source": [
    "question = \"tell me about Puerto Rico?\"\n",
    "\n",
    "docs = retriever.get_relevant_documents(query=question)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kSu1GsFfAqCd"
   },
   "source": [
    "From this we get a variety of docs retrieved by each of our queries independently. By default the `retriever` is returning `3` docs for each query — totalling `9` documents — however, as there is some overlap we actually return `6` unique docs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ce5WBh6MFltP",
    "outputId": "f7b06949-e2a6-472e-eaf9-e712dc4bcca2"
   },
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KLMwfZPfBF89"
   },
   "source": [
    "## Adding the Generation in RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X79eNNL_BM4G"
   },
   "source": [
    "So far we've built a multi-query powered **R**etrieval **A**ugmentation chain. Now, we need to add **G**eneration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jNnXYOtqypiz"
   },
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "QA_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"query\", \"contexts\"],\n",
    "    template=\"\"\"You are a helpful assistant who answers user queries using the\n",
    "    contexts provided. If the question cannot be answered using the information\n",
    "    provided say \"I don't know\".\n",
    "\n",
    "    Contexts:\n",
    "    {contexts}\n",
    "\n",
    "    Question: {query}\"\"\",\n",
    ")\n",
    "\n",
    "# Chain\n",
    "qa_chain = LLMChain(llm=llm, prompt=QA_PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 123
    },
    "id": "h6GVEZkhytdM",
    "outputId": "f03086b8-8d30-4d6e-a723-833ffecbcf8e"
   },
   "outputs": [],
   "source": [
    "out = qa_chain(\n",
    "    inputs={\n",
    "        \"query\": question,\n",
    "        \"contexts\": \"\\n---\\n\".join([d.page_content for d in docs])\n",
    "    }\n",
    ")\n",
    "out[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KemgDCg8DkgE"
   },
   "source": [
    "## Chaining Everything with a SequentialChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kTbLlWgEEII1"
   },
   "source": [
    "We can pull together the logic above into a function or set of methods, whatever is prefered — however if we'd like to use LangChain's approach to this we must \"chain\" together multiple chains. The first retrieval component is (1) not a chain per se, and (2) requires processing of the output. To do that, and fit with LangChain's \"chaining chains\" approach, we setup the _retrieval_ component within a `TransformChain`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BpFmiRtYDpHp"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import TransformChain\n",
    "\n",
    "def retrieval_transform(inputs: dict) -> dict:\n",
    "    docs = retriever.get_relevant_documents(query=inputs[\"question\"])\n",
    "    docs = [d.page_content for d in docs]\n",
    "    docs_dict = {\n",
    "        \"query\": inputs[\"question\"],\n",
    "        \"contexts\": \"\\n---\\n\".join(docs)\n",
    "    }\n",
    "    return docs_dict\n",
    "\n",
    "retrieval_chain = TransformChain(\n",
    "    input_variables=[\"question\"],\n",
    "    output_variables=[\"query\", \"contexts\"],\n",
    "    transform=retrieval_transform\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SoD45Au1Eg-r"
   },
   "source": [
    "Now we chain this with our generation step using the `SequentialChain`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "azqCwDwXEkDT"
   },
   "outputs": [],
   "source": [
    "from langchain.chains import SequentialChain\n",
    "\n",
    "rag_chain = SequentialChain(\n",
    "    chains=[retrieval_chain, qa_chain],\n",
    "    input_variables=[\"question\"],  # we need to name differently to output \"query\"\n",
    "    output_variables=[\"query\", \"contexts\", \"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xpB2aWV4ESzf"
   },
   "source": [
    "Then we perform the full RAG pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 161
    },
    "id": "JvJbUaLqFRG2",
    "outputId": "582caa21-777a-4a01-a618-9db64185ad5e"
   },
   "outputs": [],
   "source": [
    "out = rag_chain({\"question\": question})\n",
    "out[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bLmv01geK-ZS"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vAZVPhHzLDQQ"
   },
   "source": [
    "## Custom Multiquery"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rI-KVO6zjJZw"
   },
   "source": [
    "We'll try this with two prompts, both encourage more variety in search queries.\n",
    "\n",
    "**Prompt A**\n",
    "```\n",
    "Your task is to generate 3 different search queries that aim to\n",
    "answer the user question from multiple perspectives.\n",
    "Each query MUST tackle the question from a different viewpoint,\n",
    "we want to get a variety of RELEVANT search results.\n",
    "Provide these alternative questions separated by newlines.\n",
    "Original question: {question}\n",
    "```\n",
    "\n",
    "\n",
    "**Prompt B**\n",
    "```\n",
    "Your task is to generate 3 different search queries that aim to\n",
    "answer the user question from multiple perspectives. The user questions\n",
    "are focused on Large Language Models, Machine Learning, and related\n",
    "disciplines.\n",
    "Each query MUST tackle the question from a different viewpoint, we\n",
    "want to get a variety of RELEVANT search results.\n",
    "Provide these alternative questions separated by newlines.\n",
    "Original question: {question}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install openai==0.28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4IlEnYeKLFzh"
   },
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from langchain.chains import LLMChain\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.output_parsers import PydanticOutputParser\n",
    "\n",
    "\n",
    "# Output parser will split the LLM result into a list of queries\n",
    "class LineList(BaseModel):\n",
    "    # \"lines\" is the key (attribute name) of the parsed output\n",
    "    lines: List[str] = Field(description=\"Lines of text\")\n",
    "\n",
    "\n",
    "class LineListOutputParser(PydanticOutputParser):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__(pydantic_object=LineList)\n",
    "\n",
    "    def parse(self, text: str) -> LineList:\n",
    "        lines = text.strip().split(\"\\n\")\n",
    "        return LineList(lines=lines)\n",
    "\n",
    "\n",
    "output_parser = LineListOutputParser()\n",
    "\n",
    "template = \"\"\"\n",
    "Your task is to generate 3 different search queries that aim to\n",
    "answer the user question from multiple perspectives. The user questions\n",
    "are focused on traveling to Puerto Rico and visiting the most iconic places in the island. You must make sure to provide a variety of search queries that cover different\n",
    "options regarding the user itinerary.\n",
    "Each query MUST tackle the question from a different viewpoint, we\n",
    "want to get a variety of RELEVANT search results.\n",
    "Provide these alternative questions separated by newlines.\n",
    "Original question: {question}\n",
    "\"\"\"\n",
    "\n",
    "QUERY_PROMPT = PromptTemplate(\n",
    "    input_variables=[\"question\"],\n",
    "    template=template,\n",
    ")\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class Runnable(ABC):\n",
    "    @abstractmethod\n",
    "    def run(self, input_data):\n",
    "        pass\n",
    "\n",
    "import openai\n",
    "\n",
    "class ChatOpenAI(Runnable):\n",
    "    def __init__(self, temperature, openai_api_key):\n",
    "        self.temperature = temperature\n",
    "        self.openai_api_key = openai_api_key\n",
    "\n",
    "    def run(self, input_data):\n",
    "        openai.api_key = self.openai_api_key\n",
    "        response = openai.Completion.create(\n",
    "            engine=\"davinci\",\n",
    "            prompt=input_data,\n",
    "            max_tokens=100\n",
    "        )\n",
    "        return response.choices[0].text.strip()\n",
    "\n",
    "user_input = \"Tell me about Puerto Rico?\"\n",
    "# New code to replace the use of openai.Completion.create\n",
    "openai.api_key = \"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\" \n",
    "# Generate a response using the travel agent engine model\n",
    "response = openai.ChatCompletion.create(\n",
    "    model=\"gpt-4o\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"Welcome to the Travel Agent Bot. I can help you plan your trip to Puerto Rico.\"},\n",
    "        {\"role\": \"user\", \"content\": user_input}\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Extract the generated response from the API call\n",
    "generated_response = response['choices'][0]['message']['content']\n",
    "print(generated_response)\n",
    "\n",
    "\n",
    "# Create an instance of ChatOpenAI and pass the user input to run the chatbot\n",
    "#llm = ChatOpenAI(temperature=0, openai_api_key=OPENAI_API_KEY)\n",
    "#llm.run(user_input)\n",
    "\n",
    "\n",
    "# Now llm can be used as an instance of Runnable in LLMChain\n",
    "#llm_chain = LLMChain(llm=llm, prompt=QUERY_PROMPT, output_parser=output_parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0CgduNJWLBez",
    "outputId": "7ffee6c2-27b4-4bdf-8c79-7effd27e3cd4"
   },
   "outputs": [],
   "source": [
    "# Run\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.vectorstores import Pinecone\n",
    "import vectorstore\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains import TransformChain\n",
    "from langchain.chains import SequentialChain\n",
    "\n",
    "from vectorstore import Retriever\n",
    "retriever = Retriever(llm_chain=llm_chain, parser_key=\"lines\")\n",
    "retriever = MultiQueryRetriever(\n",
    "   #retriever=vectorstore.as_retriever(), llm_chain=llm_chain, parser_key=\"lines\"\n",
    "   retriever = vectorstore.Retriever(llm_chain=llm_chain, parser_key=\"lines\")\n",
    ")  # \"lines\" is the key (attribute name) of the parsed output\n",
    "\n",
    "# Results\n",
    "docs = retriever.get_relevant_documents(\n",
    "    query=question\n",
    ")\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PSySsaDKMK1i",
    "outputId": "e6f95abd-99fc-4576-d1f4-5fd4c21c70ab"
   },
   "outputs": [],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F4q65OEiizU2"
   },
   "source": [
    "Putting this together in another `SequentialChain`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LTRjTIKzi2-g"
   },
   "outputs": [],
   "source": [
    "retrieval_chain = TransformChain(\n",
    "    input_variables=[\"question\"],\n",
    "    output_variables=[\"query\", \"contexts\"],\n",
    "    transform=retrieval_transform\n",
    ")\n",
    "\n",
    "rag_chain = SequentialChain(\n",
    "    chains=[retrieval_chain, qa_chain],\n",
    "    input_variables=[\"question\"],  # we need to name differently to output \"query\"\n",
    "    output_variables=[\"query\", \"contexts\", \"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rda74xhpjE6A"
   },
   "source": [
    "And asking again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9UcBY71cjGgX"
   },
   "outputs": [],
   "source": [
    "out = rag_chain({\"question\": question})\n",
    "out[\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8jULksgk7gLA"
   },
   "source": [
    "After finishing, delete your Pinecone index to save resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc.delete_index(index_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
