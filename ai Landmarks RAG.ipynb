{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import OpenAIEmbeddings, HuggingFaceEmbeddings\n",
    "from langchain.schema import Document\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())\n",
    "\n",
    "OPENAI_API_KEY  = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **RAG Delition Code if necessary to Fresh Start**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Step 1: Release the database with the following code*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = None  # Release the vector database\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Step 2: Restart the Kernel associated to the use of the Vectorstore database*\n",
    "\n",
    "*Step 3: Delete the vectorstore by running the following code*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "VECTORSTORE_PATH = r\"C:\\chromadb\\landmarks_db\"  # Update to the actual path\n",
    "\n",
    "# Delete the existing database folder\n",
    "if os.path.exists(VECTORSTORE_PATH):\n",
    "    shutil.rmtree(VECTORSTORE_PATH)\n",
    "    print(\"Previous RAG deleted successfully.\")\n",
    "else:\n",
    "    print(\"No existing RAG found. Proceeding with a fresh build.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 1: Analyze Document Lengths & Metadata Completeness**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# Define the path to final cleaned landmark data\n",
    "cleaned_data_path = \n",
    "\n",
    "# Collect document statistics\n",
    "lengths = []\n",
    "geo_metadata_missing = 0\n",
    "total_files = 0\n",
    "\n",
    "for file in os.listdir(cleaned_data_path):\n",
    "    file_path = os.path.join(cleaned_data_path, file)\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        landmark_data = json.load(f)\n",
    "\n",
    "    text_content = landmark_data.get(\"main_content\", \"\")\n",
    "    lengths.append(len(text_content.split()))  # Count words\n",
    "    total_files += 1\n",
    "\n",
    "    if not landmark_data.get(\"geo_metadata\"):\n",
    "        geo_metadata_missing += 1\n",
    "\n",
    "# Compute statistics\n",
    "avg_length = np.mean(lengths)\n",
    "max_length = np.max(lengths)\n",
    "min_length = np.min(lengths)\n",
    "std_dev = np.std(lengths)\n",
    "\n",
    "print(f\"Total landmarks processed: {total_files}\")\n",
    "print(f\"Average document length: {avg_length:.2f} words\")\n",
    "print(f\"Max length: {max_length} words | Min length: {min_length} words\")\n",
    "print(f\"Standard deviation of lengths: {std_dev:.2f}\")\n",
    "print(f\"Landmarks missing geo_metadata: {geo_metadata_missing} ({geo_metadata_missing / total_files:.2%})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 2: Data Chunking, embedding and into DB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the paths\n",
    "cleaned_data_path = r\n",
    "vectorstore_path = r\"C:\\\\chromadb\\\\landmarks_db\"\n",
    "\n",
    "# Initialize OpenAI embeddings\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# Initialize text splitter\n",
    "chunk_size = 1024\n",
    "chunk_overlap = 0\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "# Prepare documents for indexing\n",
    "documents = []\n",
    "for file in os.listdir(cleaned_data_path):\n",
    "    file_path = os.path.join(cleaned_data_path, file)\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        landmark_data = json.load(f)\n",
    "\n",
    "    # Extract and normalize metadata\n",
    "    geo_metadata = landmark_data.get(\"geo_metadata\", {}) or {}\n",
    "    title = geo_metadata.get(\"title\", \"Unknown Title\")\n",
    "    coordinates = geo_metadata.get(\"coordinates\", [None, None])\n",
    "    categories = \", \".join(landmark_data.get(\"categories\", []))  # Convert list to string\n",
    "    relevant_links = \", \".join(landmark_data.get(\"relevant_links\", []))  # Convert list to string\n",
    "\n",
    "    main_content = landmark_data.get(\"main_content\", \"\")\n",
    "\n",
    "    # Convert coordinates into a string format\n",
    "    coordinates_str = f\"{coordinates[0]}, {coordinates[1]}\" if coordinates[0] is not None else \"Unknown\"\n",
    "\n",
    "    # Split main content into chunks\n",
    "    chunks = text_splitter.split_text(main_content)\n",
    "\n",
    "    for chunk in chunks:\n",
    "        document = {\n",
    "            \"content\": chunk,\n",
    "            \"metadata\": {\n",
    "                \"title\": title,\n",
    "                \"coordinates\": coordinates_str,  # Coordinates as a string\n",
    "                \"categories\": categories,  # Converted list to string\n",
    "                \"relevant_links\": relevant_links  # Converted list to string\n",
    "            }\n",
    "        }\n",
    "        documents.append(document)\n",
    "\n",
    "# Initialize ChromaDB\n",
    "if not os.path.exists(vectorstore_path):\n",
    "    os.makedirs(vectorstore_path)\n",
    "\n",
    "vectorstore = Chroma(persist_directory=vectorstore_path, embedding_function=embeddings)\n",
    "\n",
    "# Add documents to the vector database\n",
    "for doc in documents:\n",
    "    vectorstore.add_texts(texts=[doc[\"content\"]], metadatas=[doc[\"metadata\"]])\n",
    "\n",
    "print(\"‚úÖ RAG setup for landmarks completed. Vector database is ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **New Approach for RAG System Creation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **This approach does not work**\n",
    "\n",
    "import os\n",
    "import json\n",
    "import chromadb\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Define paths\n",
    "DATA_PATH = r\"C:\\Users\\larry\\OneDrive\\Documents\\GitHub\\project-aieng-interactive-travel-planner\\data\\landmarks_cleaning\\10Feb25_final_step02_links_clean\"\n",
    "VECTORSTORE_PATH = r\"C:\\chromadb\\landmarks_db\"\n",
    "\n",
    "# Initialize embeddings and vectorstore\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = Chroma(persist_directory=VECTORSTORE_PATH, embedding_function=embeddings)\n",
    "\n",
    "# Initialize chunking strategy\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "\n",
    "# Process all .txt files and add to vectorstore\n",
    "for file_name in os.listdir(DATA_PATH):\n",
    "    if file_name.endswith(\".txt\"):\n",
    "        file_path = os.path.join(DATA_PATH, file_name)\n",
    "\n",
    "        # Read JSON metadata file\n",
    "        json_file_path = file_path.replace(\".txt\", \".json\")  # Metadata should be in a .json file\n",
    "        if not os.path.exists(json_file_path):\n",
    "            print(f\"Metadata file missing for {file_name}. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        with open(json_file_path, \"r\", encoding=\"utf-8\") as meta_file:\n",
    "            metadata = json.load(meta_file)\n",
    "\n",
    "        # Extract metadata values\n",
    "        main_content = metadata.get(\"main_content\", \"\")  # Corrected content retrieval\n",
    "        geo_metadata = metadata.get(\"geo_metadata\", {}) or {}  # Ensure it's a dictionary\n",
    "        title = geo_metadata.get(\"title\", \"Unknown Title\")\n",
    "        coordinates = geo_metadata.get(\"coordinates\", [None, None])  # Ensure it's always a list\n",
    "        categories = \", \".join(metadata.get(\"categories\", []))  # Convert list to string\n",
    "        relevant_links = \", \".join(metadata.get(\"relevant_links\", []))  # Convert list to string\n",
    "\n",
    "        # Chunk text for better retrieval\n",
    "        chunks = text_splitter.split_text(main_content)\n",
    "\n",
    "        for chunk in chunks:\n",
    "            vectorstore.add_texts(\n",
    "                texts=[chunk],\n",
    "                metadatas=[{\n",
    "                    \"title\": title,\n",
    "                    \"content\": chunk,\n",
    "                    \"coordinates\": f\"{coordinates[0]}, {coordinates[1]}\",\n",
    "                    \"categories\": categories,\n",
    "                    \"relevant_links\": relevant_links\n",
    "                }]\n",
    "            )\n",
    "\n",
    "print(\"Landmark RAG successfully rebuilt with structured metadata & optimized chunking!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step No. 1: Metadata Reading and Extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from langchain.schema import Document\n",
    "\n",
    "DATA_PATH = \n",
    "def process_txt_file(filepath):\n",
    "    \"\"\" Extract metadata and content from a .txt file \"\"\"\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.loads(f.read())  # Assuming JSON structure inside .txt\n",
    "\n",
    "    # Extract content\n",
    "    content = data.get(\"main_content\", \"\")\n",
    "\n",
    "    # Extract metadata\n",
    "    geo_metadata = data.get(\"geo_metadata\", {}) or {}\n",
    "    title = geo_metadata.get(\"title\", \"Unknown Title\")\n",
    "    coordinates = geo_metadata.get(\"coordinates\", [None, None])\n",
    "\n",
    "    # Convert coordinates to a string\n",
    "    coordinates_str = f\"{coordinates[0]}, {coordinates[1]}\" if all(coordinates) else \"Unknown\"\n",
    "\n",
    "    # Extract categories\n",
    "    categories = \", \".join(data.get(\"categories\", []))  \n",
    "\n",
    "    # Extract relevant links\n",
    "    relevant_links = \", \".join(data.get(\"relevant_links\", []))  \n",
    "\n",
    "    return Document(\n",
    "        page_content=content,\n",
    "        metadata={\n",
    "            \"title\": title,\n",
    "            \"coordinates\": coordinates_str,\n",
    "            \"categories\": categories,\n",
    "            \"relevant_links\": relevant_links\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Process all .txt files\n",
    "documents = []\n",
    "for filename in os.listdir(DATA_PATH):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        filepath = os.path.join(DATA_PATH, filename)\n",
    "        doc = process_txt_file(filepath)\n",
    "        documents.append(doc)\n",
    "\n",
    "# Check some processed documents\n",
    "print(\"Sample Processed Docs:\", documents[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Define the output file path\n",
    "output_file = \n",
    "\n",
    "# Convert Document objects to a serializable format\n",
    "serializable_docs = [\n",
    "    {\n",
    "        \"metadata\": doc.metadata,\n",
    "        \"page_content\": doc.page_content\n",
    "    }\n",
    "    for doc in documents\n",
    "]\n",
    "\n",
    "# Save to JSON file\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(serializable_docs, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"Successfully saved {len(documents)} documents to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in documents:\n",
    "    if \"Cerro\" in doc.metadata[\"title\"]:\n",
    "        print(f\"Title: {doc.metadata['title']}\\nCategories: {doc.metadata['categories']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step No. 1a: Updating Documents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "processed_docs = documents.copy()  # Avoid modifying the original list\n",
    "\n",
    "# Function to update categories properly\n",
    "def update_landmark_categories(processed_docs, missing_landmarks, educational_landmarks):\n",
    "    \"\"\"\n",
    "    Updates missing landmark categories by assigning proper default categories.\n",
    "    Appends 'Educational Institution' to specific landmarks.\n",
    "\n",
    "    Args:\n",
    "        processed_docs (list): List of processed document dictionaries.\n",
    "        missing_landmarks (dict): Dictionary with landmark titles as keys and default categories as values.\n",
    "        educational_landmarks (list): List of landmarks that should include 'Educational Institution'.\n",
    "\n",
    "    Returns:\n",
    "        list: Updated processed documents.\n",
    "    \"\"\"\n",
    "    updated_docs = []\n",
    "    processed_docs = documents.copy()  # Avoid modifying the original list\n",
    "\n",
    "    for doc in processed_docs:\n",
    "        title = doc.metadata.get(\"title\", \"\").strip()\n",
    "        categories = set(doc.metadata.get(\"categories\", \"\").split(\", \"))\n",
    "\n",
    "        # Assign missing categories\n",
    "        if title in missing_landmarks:\n",
    "            categories.update(missing_landmarks[title])  # Add suggested categories\n",
    "        \n",
    "        # Append 'Educational Institution' where necessary\n",
    "        if title in educational_landmarks:\n",
    "            categories.add(\"Educational Institution\")\n",
    "        \n",
    "        # Ensure unique categories and update document metadata\n",
    "        doc.metadata[\"categories\"] = \", \".join(sorted(filter(None, categories)))  # Sort for consistency\n",
    "        updated_docs.append(doc)\n",
    "    \n",
    "    return updated_docs\n",
    "\n",
    "# Dictionary of landmarks with missing categories and their proposed categories\n",
    "missing_landmarks = {\n",
    "    \"Braulio Castillo\": [\"Culture\"],\n",
    "    \"Carra√≠zo Dam\": [\"Outdoor\"],\n",
    "    \"Casa Franceschi Antongiorgi\": [\"Culture\", \"Family-Friendly\"],\n",
    "    \"Faro del Castillo San Felipe del Morro\": [\"Culture\", \"Outdoor\"],\n",
    "    \"Cementerio Civil de Ponce\": [\"Religion\", \"Culture\"],\n",
    "    \"Cerro Morales (utuado, Puerto Rico)\": [\"Outdoor\", \"Adventure\"],\n",
    "    \"Cerro Rosa\": [\"Outdoor\", \"Adventure\"],\n",
    "    \"Coliseo Manuel Iguina\": [\"Sports\", \"Culture\"],\n",
    "    \"Dos Bocas Lake\": [\"Outdoor\", \"Water Activities\"],\n",
    "    \"Edificio Comunidad De Orgullo Gay De Puerto Rico\": [\"Culture\", \"LGBTQ+\"],\n",
    "    \"Edificio Del Valle\": [\"Culture\", \"Architecture\"],\n",
    "    \"Edificio Oliver\": [\"Culture\", \"Architecture\"],\n",
    "    \"Edificio Victory Garden\": [\"Culture\", \"Architecture\"],\n",
    "    \"El Gigante Dormido\": [\"Outdoor\", \"Adventure\"],\n",
    "    \"El Monumento De La Recordaci√≥n\": [\"Culture\", \"History\"],\n",
    "    \"Escuela Brambaugh\": [\"Culture\", \"History\"],\n",
    "    \"Estadio Country Club\": [\"Sports\", \"Culture\"],\n",
    "    \"Cardona Island Light\": [\"Culture\", \"Outdoor\"],\n",
    "    \"Filardi House\": [\"Culture\", \"Architecture\"],\n",
    "    \"Fuerte De La Concepci√≥n\": [\"Culture\", \"History\"],\n",
    "    \"Gu√°nica Light\": [\"Culture\", \"Outdoor\"],\n",
    "    \"G√≥mez Residence\": [\"Culture\", \"Architecture\"],\n",
    "    \"Hacienda Santa Rita\": [\"Culture\", \"History\"],\n",
    "    \"Hacienda San Francisco\": [\"Culture\", \"History\"],\n",
    "    \"House At 659 Concordia Street\": [\"Culture\", \"Architecture\"],\n",
    "    \"House At 659 La Paz Street\": [\"Culture\", \"Architecture\"],\n",
    "    \"House At 663 La Paz Street\": [\"Culture\", \"Architecture\"],\n",
    "    \"Lago Dos Bocas\": [\"Outdoor\", \"Water Activities\"],\n",
    "    \"Las Caba√±as Bridge\": [\"Culture\", \"Architecture\"],\n",
    "    \"La Giralda (San Juan, Puerto Rico)\": [\"Culture\", \"Architecture\"],\n",
    "    \"La Liendre Bridge\": [\"Culture\", \"Architecture\"],\n",
    "    \"Logia Mas√≥nica Hijos De La Luz\": [\"Culture\", \"History\"],\n",
    "    \"Los Tres Picachos\": [\"Outdoor\", \"Adventure\"],\n",
    "    \"Los T√∫neles Subterr√°neos De San Germ√°n\": [\"Culture\", \"History\"],\n",
    "    \"Lo√≠za Lake\": [\"Outdoor\", \"Water Activities\"],\n",
    "    \"Luis A. Ferr√© United States Courthouse And Post Office Building\": [\"Culture\", \"Government\"],\n",
    "    \"Mario Morales Coliseum\": [\"Sports\", \"Culture\"],\n",
    "    \"Monte Jayuya\": [\"Outdoor\", \"Adventure\"],\n",
    "    \"Monumento al J√≠baro Puertorrique√±o\": [\"Culture\", \"History\"],\n",
    "    \"Monumento a los h√©roes de El Polvor√≠n (obelisk)\": [\"Culture\", \"History\"],\n",
    "    \"Parque De Bombas Maximiliano Merced\": [\"Culture\", \"History\"],\n",
    "    \"Paseo V√≠ctor Rojas\": [\"Culture\", \"Family-Friendly\"],\n",
    "    \"Puente Blanco\": [\"Culture\", \"Architecture\"],\n",
    "    \"Puerto Del Rey Marina\": [\"Outdoor\", \"Water Activities\"],\n",
    "    \"Roberto Clemente Stadium\": [\"Sports\", \"Culture\"],\n",
    "    \"Rum Planetarium\": [\"Culture\", \"Science\", \"Educational Institution\"],\n",
    "    \"Sabana Grande Masonic Cemetery\": [\"Religion\", \"Culture\"],\n",
    "    \"Teodoro Moscoso Bridge\": [\"Culture\", \"Infrastructure\"],\n",
    "    \"U.s. Post Office And Courthouse (mayag√ºez, Puerto Rico)\": [\"Culture\", \"Government\"],\n",
    "    \"University Of Puerto Rico At Cayey\": [\"Educational Institution\"]\n",
    "}\n",
    "\n",
    "# List of landmarks that should include \"Educational Institution\"\n",
    "educational_landmarks = [\n",
    "    \"University Of Puerto Rico, R√≠o Piedras Campus\",\n",
    "    \"University Of Puerto Rico School Of Medicine\",\n",
    "    \"University Of Puerto Rico, Medical Sciences Campus\",\n",
    "    \"University Of Puerto Rico At Mayag√ºez\",\n",
    "    \"Center For Advanced Studies On Puerto Rico And The Caribbean\",\n",
    "    \"Universidad Del Sagrado Coraz√≥n\",\n",
    "    \"Interamerican University Of Puerto Rico At Ponce\",\n",
    "    \"Interamerican University Of Puerto Rico\",\n",
    "    \"University Of Puerto Rico At Humacao\",\n",
    "    \"University Of Puerto Rico At Ponce\",\n",
    "    \"University Gardens High School\",\n",
    "    \"University High School (San Juan)\",\n",
    "    \"Polytechnic University Of Puerto Rico\",\n",
    "    \"School Of Tropical Medicine (puerto Rico)\",\n",
    "    \"Mizpa Pentecostal University\",\n",
    "    \"Pontificia Universidad Cat√≥lica de Puerto Rico\",\n",
    "    \"Ponce School Of Medicine\",\n",
    "    \"Escuela De Artes Pl√°sticas Y Dise√±o De Puerto Rico\",\n",
    "    \"Academia Interamericana Metro\",\n",
    "    \"Academia del Perpetuo Socorro\",\n",
    "    \"Escuela Brambaugh\",\n",
    "    \"Conservatory Of Music Of Puerto Rico\",\n",
    "    \"Saint John's School (San Juan)\",\n",
    "    \"Arecibo Observatory\",\n",
    "    \"Albizu University\",\n",
    "    \"Academia San Jorge\",\n",
    "    \"Academia Maria Reina\",\n",
    "    \"Ana G. M√©ndez University\",\n",
    "    \"Colegio San Conrado\",\n",
    "    \"Ponce High School\",\n",
    "    \"Central High School (san Juan, Puerto Rico)\",\n",
    "    \"Colegio Ponce√±o\",\n",
    "    \"Biblioteca Carnegie\",\n",
    "    \"Albergue Ol√≠mpico\"\n",
    "]\n",
    "\n",
    "# Apply the updates\n",
    "updated_docs = update_landmark_categories(processed_docs, missing_landmarks, educational_landmarks)\n",
    "\n",
    "# Display some sample results\n",
    "for doc in updated_docs[:5]:  # Show first 5 updated docs\n",
    "    print(f\"Title: {doc.metadata['title']}\\nCategories: {doc.metadata['categories']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Sample Processed Docs:\", updated_docs[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in updated_docs:\n",
    "    if \"University\" in doc.metadata[\"title\"]:\n",
    "        print(f\"Title: {doc.metadata['title']}\\nCategories: {doc.metadata['categories']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in updated_docs:\n",
    "    if \"Coliseo\" in doc.metadata[\"title\"]:\n",
    "        print(f\"Title: {doc.metadata['title']}\\nCategories: {doc.metadata['categories']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in updated_docs:\n",
    "    if \"Lago\" in doc.metadata[\"title\"]:\n",
    "        print(f\"Title: {doc.metadata['title']}\\nCategories: {doc.metadata['categories']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Define the output file path\n",
    "output_file = \n",
    "\n",
    "# Convert Document objects to a serializable format\n",
    "serializable_docs = [\n",
    "    {\n",
    "        \"metadata\": doc.metadata,\n",
    "        \"page_content\": doc.page_content\n",
    "    }\n",
    "    for doc in updated_docs\n",
    "]\n",
    "\n",
    "# Save to JSON file\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(serializable_docs, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"Successfully saved {len(updated_docs)} documents to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Define the output file path\n",
    "output_file = \n",
    "# Load from JSON file\n",
    "with open(output_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    loaded_docs = json.load(f)\n",
    "\n",
    "# Convert back to Document objects\n",
    "restored_docs = [\n",
    "    Document(metadata=doc[\"metadata\"], page_content=doc[\"page_content\"])\n",
    "    for doc in loaded_docs\n",
    "]\n",
    "\n",
    "print(f\"Successfully loaded {len(restored_docs)} documents from {output_file}\")\n",
    "\n",
    "updated_docs=restored_docs.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step No. 2: Chunking Documents**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Adaptive chunking strategy\n",
    "SMALL_DOC_THRESHOLD = 500\n",
    "MEDIUM_DOC_THRESHOLD = 2000\n",
    "\n",
    "# Define chunking parameters dynamically\n",
    "def get_chunk_parameters(doc_length):\n",
    "    if doc_length <= SMALL_DOC_THRESHOLD:\n",
    "        return None  # No chunking needed\n",
    "    elif doc_length <= MEDIUM_DOC_THRESHOLD:\n",
    "        return {\"chunk_size\": 500, \"chunk_overlap\": 50}\n",
    "    else:\n",
    "        return {\"chunk_size\": 1000, \"chunk_overlap\": 100}\n",
    "\n",
    "chunked_documents = []\n",
    "\n",
    "processed_docs = updated_docs.copy()  # Assuming we have processed the updated_docs from previous step\n",
    "\n",
    "for doc in processed_docs:\n",
    "    title = doc.metadata.get(\"title\", \"Unknown Title\")\n",
    "    coordinates = doc.metadata.get(\"coordinates\", \"Unknown\")\n",
    "    categories = doc.metadata.get(\"categories\", \"Unknown\")\n",
    "    relevant_links = doc.metadata.get(\"relevant_links\", \"\")\n",
    "\n",
    "    doc_length = len(doc.page_content.split())\n",
    "\n",
    "    chunk_params = get_chunk_parameters(doc_length)\n",
    "    if chunk_params is None:\n",
    "        # Small docs, no chunking, add as-is\n",
    "        chunked_documents.append({\n",
    "            \"content\": doc.page_content,\n",
    "            \"metadata\": {\n",
    "                \"title\": title,\n",
    "                \"coordinates\": coordinates,\n",
    "                \"categories\": categories,\n",
    "                \"relevant_links\": relevant_links\n",
    "            }\n",
    "        })\n",
    "    else:\n",
    "        # Apply chunking\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_params[\"chunk_size\"],\n",
    "            chunk_overlap=chunk_params[\"chunk_overlap\"]\n",
    "        )\n",
    "        chunks = text_splitter.split_text(doc.page_content)\n",
    "\n",
    "        for chunk in chunks:\n",
    "            chunked_documents.append({\n",
    "                \"content\": chunk,\n",
    "                \"metadata\": {\n",
    "                    \"title\": title,\n",
    "                    \"coordinates\": coordinates,\n",
    "                    \"categories\": categories,\n",
    "                    \"relevant_links\": relevant_links\n",
    "                }\n",
    "            })\n",
    "\n",
    "print(f\"Total chunks created: {len(chunked_documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step No. 3: Qucik Chunks Verification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check first 5 chunks for verification\n",
    "for i, chunk in enumerate(chunked_documents[:5]):\n",
    "    print(f\"üîπ Chunk {i+1}: {len(chunk['content'].split())} words | Title: {chunk['metadata']['title']}\")\n",
    "    print(chunk['content'][:300])  # Preview first 300 chars\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step No. 4: Categories Distribution Verification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check categories distribution\n",
    "from collections import Counter\n",
    "category_counts = Counter([doc['metadata']['categories'] for doc in chunked_documents])\n",
    "print(category_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check which documents are missing categories\n",
    "missing_categories = [doc['metadata']['title'] for doc in chunked_documents if not doc['metadata']['categories']]\n",
    "print(\"Landmarks Missing Categories:\", len(missing_categories))\n",
    "print(missing_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Define the output file path\n",
    "output_file = \n",
    "\n",
    "# Save chunked documents as JSON\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(chunked_documents, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"Successfully saved {len(chunked_documents)} chunks to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loaded {len(chunked_documents)} chunks\")\n",
    "print(\"Sample Chunk:\", chunked_documents[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step No. 5: Creation of the Vectorstore with ChromaDB**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U langchain langchain-community langchain-chroma chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import chromadb\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Initialize the OpenAI embedding model\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")  # Adjust model if needed\n",
    "\n",
    "# Define the ChromaDB storage path\n",
    "chroma_db_path = \n",
    "\n",
    "# Initialize a persistent ChromaDB client\n",
    "chroma_client = chromadb.PersistentClient(path=chroma_db_path)\n",
    "\n",
    "# Convert chunked documents into LangChain Document objects\n",
    "docs = [\n",
    "    Document(\n",
    "        page_content=chunk[\"content\"],\n",
    "        metadata=chunk[\"metadata\"]\n",
    "    ) \n",
    "    for chunk in chunked_documents\n",
    "]\n",
    "\n",
    "# Create and store the vector database with ChromaDB\n",
    "vectorstore = Chroma.from_documents(\n",
    "    documents=docs, \n",
    "    embedding=embedding_model, \n",
    "    persist_directory=chroma_db_path\n",
    ")\n",
    "\n",
    "print(\"ChromaDB Vectorstore successfully created and stored at:\", chroma_db_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step No. 6: Perform a Retrieval Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the stored vectorstore from ChromaDB\n",
    "vectorstore = Chroma(\n",
    "    persist_directory=chroma_db_path, \n",
    "    embedding_function=embedding_model\n",
    ")\n",
    "\n",
    "# Perform a test query\n",
    "query = \"historical landmarks in San Juan\"\n",
    "results = vectorstore.similarity_search(query, k=5)  # Retrieve top 5 most relevant results\n",
    "\n",
    "# Display retrieved results\n",
    "print(\"\\n**Search Results:**\\n\")\n",
    "for idx, doc in enumerate(results):\n",
    "    print(f\"Result {idx+1}:\")\n",
    "    print(f\"Title: {doc.metadata.get('title', 'Unknown')}\")\n",
    "    print(f\"Snippet: {doc.page_content[:300]}...\")  # Display first 300 characters\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the stored vectorstore from ChromaDB\n",
    "vectorstore = Chroma(\n",
    "    persist_directory=chroma_db_path, \n",
    "    embedding_function=embedding_model\n",
    ")\n",
    "\n",
    "# Perform a test query\n",
    "query = \"historical landmarks in San Juan\"\n",
    "results = vectorstore.similarity_search_with_score(query, k=5)\n",
    "\n",
    "# Display retrieved results with similarity scores\n",
    "print(\"\\n**Search Results with Similarity Scores:**\\n\")\n",
    "for idx, (doc, score) in enumerate(results):\n",
    "    print(f\"Result {idx+1}:\")\n",
    "    print(f\"Title: {doc.metadata.get('title', 'Unknown')}\")\n",
    "    print(f\"Similarity Score: {score:.4f}\")  # Display similarity score\n",
    "    print(f\"Snippet: {doc.page_content[:300]}...\")  # Show first 300 characters\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To verify if the embeddings are working correctly, we can embed a query and check the first few values of the embedding vector.\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "\n",
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-ada-002\")\n",
    "\n",
    "query = \"historical landmarks in San Juan\"\n",
    "query_embedding = embedding_model.embed_query(query)\n",
    "\n",
    "print(query_embedding[:5])  # Show first 5 values to verify"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Step No. 7: Inspecting the RAG Vector Database Structure and Content**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -U langchain-chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "import random\n",
    "\n",
    "# Define the path to the vectorstore (RAG system)\n",
    "VECTORSTORE_PATH = \"C:\\chromadb\\landmarks_db\"\n",
    "\n",
    "# Initialize embeddings and vectorstore\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = Chroma(persist_directory=VECTORSTORE_PATH, embedding_function=embeddings)\n",
    "\n",
    "# Check the total number of indexed documents\n",
    "num_docs = vectorstore._collection.count()\n",
    "print(f\"\\nTotal Number of Documents in RAG: {num_docs}\")\n",
    "\n",
    "# Retrieve random sample documents to inspect content and metadata\n",
    "sample_size = min(5, num_docs)  # Limit to 5 samples\n",
    "random_indices = random.sample(range(num_docs), sample_size)\n",
    "print(\"\\n=== Random Document Samples ===\")\n",
    "\n",
    "for idx in random_indices:\n",
    "    doc = vectorstore._collection.get(include=[\"metadatas\", \"documents\"], ids=[str(idx)])\n",
    "    \n",
    "    if doc and \"documents\" in doc and len(doc[\"documents\"]) > 0:\n",
    "        doc_content = doc[\"documents\"][0][:500]  # Show first 500 characters\n",
    "        doc_metadata = doc[\"metadatas\"][0] if \"metadatas\" in doc and len(doc[\"metadatas\"]) > 0 else \"No metadata\"\n",
    "        print(f\"\\n**Document {idx}**\")\n",
    "        print(f\"Title: {doc_metadata.get('title', 'Unknown')}\")\n",
    "        print(f\"Metadata: {doc_metadata}\")\n",
    "        print(f\"Content Preview: {doc_content}...\\n\")\n",
    "    else:\n",
    "        print(f\"No data found for document {idx}\")\n",
    "\n",
    "print(\"\\nRAG Database Inspection Completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_docs = vectorstore.get([\"1\", \"2\", \"3\"])  # Retrieve first few stored docs\n",
    "for doc in sample_docs:\n",
    "    print(doc.metadata)\n",
    "    print(doc.page_content[:200])  # Show first 200 characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_docs = vectorstore.get([\"1\", \"10\", \"50\"])  # Retrieve sample docs\n",
    "\n",
    "for doc in sample_docs:\n",
    "    print(\"Title:\", doc.metadata.get(\"title\", \"Unknown\"))\n",
    "    print(\"Categories:\", doc.metadata.get(\"categories\", \"Unknown\"))\n",
    "    print(\"Snippet:\", doc.page_content[:300], \"...\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve a few available documents\n",
    "doc_sample = vectorstore._collection.get(include=[\"metadatas\", \"documents\"], limit=5)  # Get first 5 docs\n",
    "\n",
    "if doc_sample and \"documents\" in doc_sample:\n",
    "    for i in range(len(doc_sample[\"documents\"])):\n",
    "        doc_content = doc_sample[\"documents\"][i][:500]  # First 500 characters\n",
    "        doc_metadata = doc_sample[\"metadatas\"][i] if \"metadatas\" in doc_sample else \"No metadata\"\n",
    "        \n",
    "        print(f\"\\n**Document {i}**\")\n",
    "        print(f\"Title: {doc_metadata.get('title', 'Unknown')}\")\n",
    "        print(f\"Metadata: {doc_metadata}\")\n",
    "        print(f\"Content Preview: {doc_content}...\\n\")\n",
    "else:\n",
    "    print(\"No documents retrieved from the collection.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve and inspect the first 5 documents stored in the database\n",
    "sample_docs = vectorstore._collection.get(include=[\"documents\", \"metadatas\"], limit=5)\n",
    "print(sample_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Tell me about El Yunque National Forest.\"\n",
    "docs = vectorstore.similarity_search(query, k=5)\n",
    "\n",
    "for doc in docs:\n",
    "    print(f\"\\nTitle: {doc.metadata.get('title', 'Unknown')}\")\n",
    "    print(f\"Similarity Score: {doc.metadata.get('similarity', 'N/A')}\")\n",
    "    print(f\"Content Preview: {doc.page_content[:300]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "# Load the vectorstore\n",
    "VECTORSTORE_PATH = r\"C:\\chromadb\\landmarks_db\"\n",
    "vectorstore = chromadb.PersistentClient(path=VECTORSTORE_PATH).get_or_create_collection(name=\"landmarks\")\n",
    "\n",
    "# Get all documents\n",
    "all_documents = vectorstore.get(include=[\"documents\", \"metadatas\"])\n",
    "\n",
    "# Extract metadata\n",
    "metadata_list = all_documents[\"metadatas\"]\n",
    "\n",
    "# 1Ô∏è Check total number of indexed documents\n",
    "num_docs = len(metadata_list)\n",
    "print(f\"Total Number of Indexed Documents: {num_docs}\")\n",
    "\n",
    "# 2Ô∏è Check unique titles to detect duplicate entries\n",
    "unique_titles = set(meta.get(\"title\", \"Unknown Title\") for meta in metadata_list)\n",
    "print(f\"Unique Titles in RAG: {len(unique_titles)} (duplicates exist if this is lower than total docs)\")\n",
    "\n",
    "# 3Ô∏è Verify metadata completeness\n",
    "missing_metadata = [meta for meta in metadata_list if not all(k in meta for k in [\"title\", \"coordinates\", \"categories\"])]\n",
    "if missing_metadata:\n",
    "    print(f\"{len(missing_metadata)} documents are missing metadata fields.\")\n",
    "\n",
    "# 4Ô∏è Check category distribution\n",
    "category_counts = {}\n",
    "for meta in metadata_list:\n",
    "    categories = meta.get(\"categories\", \"Unknown\").split(\", \")\n",
    "    for category in categories:\n",
    "        category_counts[category] = category_counts.get(category, 0) + 1\n",
    "\n",
    "print(\"Category Distribution:\")\n",
    "for category, count in sorted(category_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  - {category}: {count} documents\")\n",
    "\n",
    "# 5Ô∏è Identify documents without categories\n",
    "no_category_docs = [meta[\"title\"] for meta in metadata_list if meta.get(\"categories\") in [None, \"\", \"Unknown\"]]\n",
    "if no_category_docs:\n",
    "    print(f\"{len(no_category_docs)} documents are missing categories.\")\n",
    "    print(\"Sample missing category docs:\", no_category_docs[:5])\n",
    "\n",
    "# 6Ô∏è Check if coordinates exist for all landmarks\n",
    "missing_coordinates = [meta[\"title\"] for meta in metadata_list if meta.get(\"coordinates\") in [None, \"\", \"Unknown\"]]\n",
    "if missing_coordinates:\n",
    "    print(f\"{len(missing_coordinates)} documents are missing coordinates.\")\n",
    "    print(\"Sample missing coordinate docs:\", missing_coordinates[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "query_embedding = embedding_model.embed_query(\"historical landmarks in San Juan\")\n",
    "doc_embedding = vectorstore._collection.get(ids=[\"1\"])[\"embeddings\"][0]\n",
    "\n",
    "similarity = np.dot(query_embedding, doc_embedding) / (np.linalg.norm(query_embedding) * np.linalg.norm(doc_embedding))\n",
    "\n",
    "print(f\"Cosine similarity between query and doc[1]: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"First 5 docs before indexing: {docs[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_docs = vectorstore._collection.get(include=[\"documents\", \"metadatas\"], limit=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Total documents in vectorstore: {vectorstore._collection.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve a sample of stored documents\n",
    "sample_docs = vectorstore._collection.get(include=[\"documents\", \"metadatas\"], limit=5)\n",
    "\n",
    "# Check if documents exist\n",
    "if \"documents\" in sample_docs and sample_docs[\"documents\"]:\n",
    "    for i in range(len(sample_docs[\"documents\"])):\n",
    "        doc_content = sample_docs[\"documents\"][i][:500]  # First 500 characters\n",
    "        doc_metadata = sample_docs[\"metadatas\"][i] if \"metadatas\" in sample_docs else \"No metadata\"\n",
    "\n",
    "        print(f\"\\n**Document {i}**\")\n",
    "        print(f\"Title: {doc_metadata.get('title', 'Unknown')}\")\n",
    "        print(f\"Metadata: {doc_metadata}\")\n",
    "        print(f\"Content Preview: {doc_content}...\\n\")\n",
    "else:\n",
    "    print(\"No documents retrieved from the collection.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Generate an embedding for a sample query\n",
    "query_embedding = embedding_model.embed_query(\"historical landmarks in San Juan\")\n",
    "\n",
    "# Retrieve embeddings for a stored document\n",
    "doc_embedding = vectorstore._collection.get(include=[\"embeddings\"], limit=1)[\"embeddings\"][0]\n",
    "\n",
    "# Compute cosine similarity\n",
    "similarity = np.dot(query_embedding, doc_embedding) / (np.linalg.norm(query_embedding) * np.linalg.norm(doc_embedding))\n",
    "\n",
    "print(f\"Cosine similarity between query and first document: {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Pre-RAG Verification Code (Full Implementation)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import pandas as pd\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "# Define the path to the vectorstore (RAG system)\n",
    "VECTORSTORE_PATH = \"/path/to/chromadb/landmarks_db\"  # Update this with the actual path\n",
    "\n",
    "# Initialize embeddings and vectorstore\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = Chroma(persist_directory=VECTORSTORE_PATH, embedding_function=embeddings)\n",
    "\n",
    "# Define test queries for RAG verification\n",
    "test_queries = [\n",
    "    \"Where is El Yunque National Forest located?\",\n",
    "    \"Tell me about Castillo San Felipe del Morro.\",\n",
    "    \"What are the most famous beaches in Puerto Rico?\",\n",
    "    \"Which landmarks should I visit in Old San Juan?\",\n",
    "    \"What are the best hiking trails in Puerto Rico?\"\n",
    "]\n",
    "\n",
    "# Initialize an empty list to store retrieval results\n",
    "retrieval_results = []\n",
    "\n",
    "# Function to check retrieval quality\n",
    "def evaluate_retrieval(query):\n",
    "    \"\"\"Retrieves documents from the RAG system and logs similarity scores.\"\"\"\n",
    "    docs = vectorstore.similarity_search(query, k=3)  # Retrieve top 3 documents\n",
    "    retrieved_chunks = []\n",
    "    \n",
    "    for doc in docs:\n",
    "        chunk_data = {\n",
    "            \"query\": query,\n",
    "            \"retrieved_text\": doc.page_content[:300] + \"...\",  # Show only first 300 chars\n",
    "            \"source_title\": doc.metadata.get(\"title\", \"Unknown\"),\n",
    "            \"similarity_score\": doc.metadata.get(\"similarity\", \"N/A\")\n",
    "        }\n",
    "        retrieved_chunks.append(chunk_data)\n",
    "    \n",
    "    return retrieved_chunks\n",
    "\n",
    "# Run retrieval tests on all queries\n",
    "for query in test_queries:\n",
    "    retrieval_results.extend(evaluate_retrieval(query))\n",
    "\n",
    "# Convert results to DataFrame for better readability\n",
    "retrieval_df = pd.DataFrame(retrieval_results)\n",
    "\n",
    "# Display verification results\n",
    "print(\"\\n=== Pre-RAG Verification Results ===\")\n",
    "print(retrieval_df.to_string(index=False))\n",
    "\n",
    "# Save results to a CSV file for further analysis\n",
    "retrieval_df.to_csv(\"rag_verification_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **RAG - Landmarks: Retreival Performance Test and Metrics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import psutil\n",
    "import chromadb\n",
    "import matplotlib.pyplot as plt\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from rouge import Rouge\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths\n",
    "VECTORSTORE_PATH = r\"C:\\chromadb\\landmarks_db\"\n",
    "TEST_SET_PATH = \n",
    "RESULTS_OUTPUT_PATH = r\n",
    "\n",
    "# Load embeddings & vectorstore\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = Chroma(persist_directory=VECTORSTORE_PATH, embedding_function=embeddings)\n",
    "\n",
    "# Load test queries\n",
    "with open(TEST_SET_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    test_queries = json.load(f)\n",
    "\n",
    "# Initialize performance log\n",
    "evaluation_results = []\n",
    "\n",
    "def measure_cpu():\n",
    "    \"\"\"Returns CPU usage as a percentage.\"\"\"\n",
    "    return psutil.cpu_percent(interval=1)\n",
    "\n",
    "def evaluate_retrieval(query):\n",
    "    \"\"\"Runs the query, retrieves documents, measures similarity, and evaluates the response.\"\"\"\n",
    "    start_time = time.time()\n",
    "    cpu_before = measure_cpu()\n",
    "\n",
    "    # Run retrieval\n",
    "    docs = vectorstore.similarity_search(query, k=3)  # Using top-k = 3\n",
    "\n",
    "    cpu_after = measure_cpu()\n",
    "    latency = time.time() - start_time\n",
    "\n",
    "    # Extract retrieved texts\n",
    "    retrieved_texts = [doc.page_content for doc in docs]\n",
    "    retrieved_titles = [doc.metadata.get(\"title\", \"Unknown\") for doc in docs]\n",
    "\n",
    "    return retrieved_texts, retrieved_titles, cpu_before, cpu_after, latency\n",
    "\n",
    "# Initialize evaluation metrics\n",
    "rouge = Rouge()\n",
    "rouge_scores, meteor_scores, bleu_scores = [], [], []\n",
    "\n",
    "# Run evaluation on all queries\n",
    "for entry in test_queries:\n",
    "    query = entry[\"query\"]\n",
    "    expected_answers = entry[\"expected_answers\"]\n",
    "\n",
    "    # Retrieve the text using the RAG retriever\n",
    "    retrieved_texts, retrieved_titles, cpu_before, cpu_after, latency = evaluate_retrieval(query)\n",
    "\n",
    "    # Tokenize the expected answers and retrieved texts for METEOR\n",
    "    expected_tokens = [word_tokenize(answer) for answer in expected_answers]\n",
    "    retrieved_tokens = word_tokenize(\" \".join(retrieved_texts))\n",
    "\n",
    "    # Calculate evaluation metrics\n",
    "    rouge_score = rouge.get_scores(\" \".join(retrieved_texts), \" \".join(expected_answers))[0][\"rouge-l\"][\"f\"]\n",
    "    meteor_score_value = meteor_score(expected_tokens, retrieved_tokens)\n",
    "    bleu_score = sentence_bleu([answer.split() for answer in expected_answers], \" \".join(retrieved_texts).split())\n",
    "\n",
    "    rouge_scores.append(rouge_score)\n",
    "    meteor_scores.append(meteor_score_value)\n",
    "    bleu_scores.append(bleu_score)\n",
    "\n",
    "    avg_rouge = sum(rouge_scores) / len(rouge_scores)\n",
    "    avg_meteor = sum(meteor_scores) / len(meteor_scores)\n",
    "    avg_bleu = sum(bleu_scores) / len(bleu_scores)\n",
    "\n",
    "    # Log results\n",
    "    result_entry = {\n",
    "        \"query\": query,\n",
    "        \"expected_answers\": expected_answers,\n",
    "        \"retrieved_texts\": retrieved_texts,\n",
    "        \"retrieved_titles\": retrieved_titles,\n",
    "        \"retrieval_scores\": {\n",
    "            \"ROUGE-L\": avg_rouge,\n",
    "            \"METEOR\": avg_meteor,\n",
    "            \"BLEU\": avg_bleu\n",
    "        },\n",
    "        \"cpu_usage\": f\"{cpu_before}% -> {cpu_after}%\",\n",
    "        \"latency\": f\"{latency:.4f} seconds\"\n",
    "    }\n",
    "    evaluation_results.append(result_entry)\n",
    "\n",
    "# Save results\n",
    "with open(RESULTS_OUTPUT_PATH, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(evaluation_results, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"RAG evaluation completed. Results saved to {RESULTS_OUTPUT_PATH}\")\n",
    "\n",
    "# Visualization (CPU vs. Latency)\n",
    "cpu_usage_data = [float(entry[\"cpu_usage\"].split(\"->\")[1].strip()[:-1]) for entry in evaluation_results]\n",
    "latency_data = [float(entry[\"latency\"].split()[0]) for entry in evaluation_results]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(cpu_usage_data, label=\"CPU Usage (%)\", marker=\"o\", linestyle=\"--\")\n",
    "plt.plot(latency_data, label=\"Latency (seconds)\", marker=\"s\", linestyle=\"-\")\n",
    "plt.xlabel(\"Test Query\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.title(\"CPU Usage vs. Latency per Query\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Visualization (Retrieval Scores)\n",
    "bleu_scores = [entry[\"retrieval_scores\"][\"BLEU\"] for entry in evaluation_results]\n",
    "rouge_scores = [entry[\"retrieval_scores\"][\"ROUGE-L\"] for entry in evaluation_results]\n",
    "meteor_scores = [entry[\"retrieval_scores\"][\"METEOR\"] for entry in evaluation_results]\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(range(len(bleu_scores)), bleu_scores, color=\"blue\", label=\"BLEU\")\n",
    "plt.bar(range(len(rouge_scores)), rouge_scores, color=\"red\", label=\"ROUGE-L\", alpha=0.7)\n",
    "plt.bar(range(len(meteor_scores)), meteor_scores, color=\"green\", label=\"METEOR\", alpha=0.7)\n",
    "plt.xlabel(\"Test Query\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.title(\"Retrieval Quality Metrics\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
