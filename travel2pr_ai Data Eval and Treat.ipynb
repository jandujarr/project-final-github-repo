{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yo ensure that all required libraries were installed correctly\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "import langchain\n",
    "import reportlab\n",
    "import qrcode\n",
    "import redis\n",
    "import celery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the directory containing the HTML files\n",
    "data_path = r\"C:\\Users\\anduj\\Desktop\\landmarks\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all files in the directory\n",
    "files = os.listdir(data_path)\n",
    "print(f\"Found {len(files)} files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print each filename\n",
    "print(\"List of files:\")\n",
    "for idx, file in enumerate(files, start=1):\n",
    "    print(f\"{idx}. {file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the first 3 files to analyze the structure\n",
    "\n",
    "for file in files[:3]:  # Read only 3 files for initial evaluation\n",
    "    file_path = os.path.join(data_path, file)\n",
    "    \n",
    "    # Read the HTML file\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        html_content = f.read()\n",
    "    \n",
    "    # Use BeautifulSoup to parse the HTML\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    \n",
    "    # Extract the main text content without HTML tags\n",
    "    extracted_text = soup.get_text(separator=\"\\n\", strip=True)\n",
    "    \n",
    "    print(f\"\\n--- {file} ---\\n\")\n",
    "    print(extracted_text[:1000])  # Display only the first 1000 characters\n",
    "    print(\"\\n\" + \"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Cleaning Test No. 01 - Relevant content extracted but relevant links were eliminated**\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the directories\n",
    "data_path = r\"C:\\Users\\larry\\OneDrive\\Documents\\GitHub\\project-aieng-interactive-travel-planner\\data\\landmarks\\data_test\"\n",
    "output_path = r\"C:\\Users\\larry\\OneDrive\\Documents\\GitHub\\project-aieng-interactive-travel-planner\\data\\landmarks\\cleaned_data\"\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "# Function to clean and extract meaningful content\n",
    "def clean_html(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        html_content = f.read()\n",
    "    \n",
    "    # Parse the HTML content with BeautifulSoup\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    \n",
    "    # Extract text from relevant tags (e.g., <p>, <h1>, <h2>)\n",
    "    paragraphs = soup.find_all(['h1', 'h2', 'p'])\n",
    "    \n",
    "    # Combine and clean extracted text\n",
    "    cleaned_text = \"\\n\".join([para.get_text(strip=True) for para in paragraphs])\n",
    "    return cleaned_text\n",
    "\n",
    "# Process and save cleaned content\n",
    "for file in os.listdir(data_path):\n",
    "    file_path = os.path.join(data_path, file)\n",
    "    cleaned_content = clean_html(file_path)\n",
    "    \n",
    "    # Save cleaned content to a new file\n",
    "    output_file_path = os.path.join(output_path, file)\n",
    "    with open(output_file_path, \"w\", encoding=\"utf-8\") as out_file:\n",
    "        out_file.write(cleaned_content)\n",
    "    \n",
    "    print(f\"Cleaned and saved: {file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **Cleaning Test No. 02 - Relevant content extracted and relevant links were include but it was found that geolocation info were eliminated**\n",
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the directories\n",
    "data_path = r\"C:\\Users\\anduj\\Desktop\\landmarks\"\n",
    "output_path = r\"C:\\Users\\anduj\\Desktop\\landmarks\"\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "# Function to validate links\n",
    "def validate_link(url):\n",
    "    try:\n",
    "        response = requests.head(url, timeout=5)  # Perform a HEAD request\n",
    "        return response.status_code == 200\n",
    "    except requests.RequestException:\n",
    "        return False\n",
    "\n",
    "# Function to clean and extract meaningful content and links\n",
    "def clean_html(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        html_content = f.read()\n",
    "    \n",
    "    # Parse the HTML content with BeautifulSoup\n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "    \n",
    "    # Extract text from relevant tags (e.g., <p>, <h1>, <h2>)\n",
    "    paragraphs = soup.find_all(['h1', 'h2', 'p'])\n",
    "    cleaned_text = \"\\n\".join([para.get_text(strip=True) for para in paragraphs])\n",
    "    \n",
    "    # Extract and validate hyperlinks\n",
    "    links = [a['href'] for a in soup.find_all('a', href=True)]\n",
    "    valid_links = [link for link in links if validate_link(link)]\n",
    "    \n",
    "    # Combine content and validated links in structured format\n",
    "    combined_content = f\"Main Content:\\n{cleaned_text}\\n\\nRelevant Links:\\n{valid_links}\"\n",
    "    return combined_content\n",
    "\n",
    "# Process and save cleaned content\n",
    "for file in os.listdir(data_path):\n",
    "    file_path = os.path.join(data_path, file)\n",
    "    cleaned_content = clean_html(file_path)\n",
    "    \n",
    "    # Save combined content (cleaned text + links) to a new file\n",
    "    output_file_path = os.path.join(output_path, file)\n",
    "    with open(output_file_path, \"w\", encoding=\"utf-8\") as out_file:\n",
    "        out_file.write(cleaned_content)\n",
    "    \n",
    "    print(f\"Cleaned and saved: {file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Defining and testing geo_metadata ectracion process**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "# Define the path to the single test file\n",
    "test_file_path = r\"C:\\Users\\anduj\\Desktop\\landmarks\"\n",
    "geo_extract_output = r\"C:\\Users\\anduj\\Desktop\\landmarks\"\n",
    "\n",
    "# Function to extract geospatial metadata without strict JSON parsing\n",
    "def extract_geo_metadata_loose(content):\n",
    "    geo_metadata = {}\n",
    "    \n",
    "    # Find coordinates\n",
    "    coordinates_pattern = r'\"coordinates\":\\s*\\[.*?\\]'\n",
    "    coordinates_match = re.search(coordinates_pattern, content)\n",
    "    if coordinates_match:\n",
    "        try:\n",
    "            # Extract and parse the coordinates\n",
    "            coordinates = json.loads(coordinates_match.group(0).split(\":\")[1].strip())\n",
    "            geo_metadata[\"coordinates\"] = coordinates\n",
    "        except json.JSONDecodeError:\n",
    "            geo_metadata[\"coordinates\"] = \"Invalid format\"\n",
    "\n",
    "    # Find title\n",
    "    title_pattern = r'\"title\":\\s*\".*?\"'\n",
    "    title_match = re.search(title_pattern, content)\n",
    "    if title_match:\n",
    "        geo_metadata[\"title\"] = title_match.group(0).split(\":\")[1].strip().strip('\"')\n",
    "\n",
    "    # Find marker type\n",
    "    marker_pattern = r'\"marker-symbol\":\\s*\".*?\"'\n",
    "    marker_match = re.search(marker_pattern, content)\n",
    "    if marker_match:\n",
    "        geo_metadata[\"type\"] = marker_match.group(0).split(\":\")[1].strip().strip('\"')\n",
    "\n",
    "    return geo_metadata\n",
    "\n",
    "# Read the test file and extract geo metadata\n",
    "with open(test_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_content = f.read()\n",
    "\n",
    "# Apply the extraction function\n",
    "geo_metadata = extract_geo_metadata_loose(raw_content)\n",
    "\n",
    "# Save the extracted metadata to a test file\n",
    "with open(geo_extract_output, \"w\", encoding=\"utf-8\") as out_file:\n",
    "    out_file.write(json.dumps(geo_metadata, indent=4))\n",
    "\n",
    "print(f\"Geo metadata extracted and saved to: {geo_extract_output}\")\n",
    "print(\"Extracted Geo Metadata:\", geo_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Cleaning Test No. 03 - Relevant content extracted, including relevant links and geolocation info.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the directories\n",
    "data_path = r\"C:\\Users\\larry\\OneDrive\\Documents\\GitHub\\project-aieng-interactive-travel-planner\\data\\landmarks\\data_test\"\n",
    "output_path = r\"C:\\Users\\larry\\OneDrive\\Documents\\GitHub\\project-aieng-interactive-travel-planner\\data\\landmarks\\cleaned_data\"\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "# Function to validate links\n",
    "def validate_link(url):\n",
    "    try:\n",
    "        response = requests.head(url, timeout=5)\n",
    "        return response.status_code == 200\n",
    "    except requests.RequestException:\n",
    "        return False\n",
    "\n",
    "# Function to extract geospatial metadata without strict JSON parsing\n",
    "def extract_geo_metadata_loose(content):\n",
    "    geo_metadata = {}\n",
    "    \n",
    "    # Find coordinates\n",
    "    coordinates_pattern = r'\"coordinates\":\\s*\\[.*?\\]'\n",
    "    coordinates_match = re.search(coordinates_pattern, content)\n",
    "    if coordinates_match:\n",
    "        try:\n",
    "            coordinates = json.loads(coordinates_match.group(0).split(\":\")[1].strip())\n",
    "            geo_metadata[\"coordinates\"] = coordinates\n",
    "        except json.JSONDecodeError:\n",
    "            geo_metadata[\"coordinates\"] = \"Invalid format\"\n",
    "\n",
    "    # Find title\n",
    "    title_pattern = r'\"title\":\\s*\".*?\"'\n",
    "    title_match = re.search(title_pattern, content)\n",
    "    if title_match:\n",
    "        geo_metadata[\"title\"] = title_match.group(0).split(\":\")[1].strip().strip('\"')\n",
    "\n",
    "    # Find marker type\n",
    "    marker_pattern = r'\"marker-symbol\":\\s*\".*?\"'\n",
    "    marker_match = re.search(marker_pattern, content)\n",
    "    if marker_match:\n",
    "        geo_metadata[\"type\"] = marker_match.group(0).split(\":\")[1].strip().strip('\"')\n",
    "\n",
    "    return geo_metadata if geo_metadata else None\n",
    "\n",
    "# Function to clean and extract meaningful content, links, and geospatial data\n",
    "def clean_html(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw_content = f.read()\n",
    "    \n",
    "    # Parse the HTML content with BeautifulSoup\n",
    "    soup = BeautifulSoup(raw_content, \"html.parser\")\n",
    "    \n",
    "    # Extract text from relevant tags (e.g., <p>, <h1>, <h2>)\n",
    "    paragraphs = soup.find_all(['h1', 'h2', 'p'])\n",
    "    cleaned_text = \"\\n\".join([para.get_text(strip=True) for para in paragraphs])\n",
    "    \n",
    "    # Extract hyperlinks\n",
    "    links = [a['href'] for a in soup.find_all('a', href=True)]\n",
    "    valid_links = [link for link in links if validate_link(link)]\n",
    "    \n",
    "    # Extract geospatial metadata\n",
    "    geo_metadata = extract_geo_metadata_loose(raw_content)\n",
    "    \n",
    "    # Combine all data\n",
    "    combined_content = {\n",
    "        \"main_content\": cleaned_text,\n",
    "        \"relevant_links\": valid_links,\n",
    "        \"geo_metadata\": geo_metadata\n",
    "    }\n",
    "    return combined_content\n",
    "\n",
    "# Process and save cleaned content\n",
    "for file in os.listdir(data_path):\n",
    "    file_path = os.path.join(data_path, file)\n",
    "    cleaned_data = clean_html(file_path)\n",
    "    \n",
    "    # Save combined content to a new file\n",
    "    output_file_path = os.path.join(output_path, file)\n",
    "    with open(output_file_path, \"w\", encoding=\"utf-8\") as out_file:\n",
    "        out_file.write(json.dumps(cleaned_data, indent=4))\n",
    "    \n",
    "    print(f\"Cleaned and saved: {file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ftfy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Cleaning Test No. 04 - Testing for special characters handling**\n",
    "# Define the directories\n",
    "cleaned_data_path = r\"C:\\\\Users\\\\larry\\\\OneDrive\\\\Documents\\\\GitHub\\\\project-aieng-interactive-travel-planner\\\\data\\\\landmarks\\\\cleaned_data\"\n",
    "final_output_path = r\"C:\\\\Users\\\\larry\\\\OneDrive\\\\Documents\\\\GitHub\\\\project-aieng-interactive-travel-planner\\\\data\\\\landmarks\\\\final_data\"\n",
    "\n",
    "# Create final output directory if it doesn't exist\n",
    "if not os.path.exists(final_output_path):\n",
    "    os.makedirs(final_output_path)\n",
    "\n",
    "# Define special character replacements\n",
    "special_chars_mapping = {\n",
    "    \"\\\\xc3\\\\xa1\": \"á\",\n",
    "    \"\\\\xc3\\\\xa9\": \"é\",\n",
    "    \"\\\\xc3\\\\xad\": \"í\",\n",
    "    \"\\\\xc3\\\\xb3\": \"ó\",\n",
    "    \"\\\\xc3\\\\xba\": \"ú\",\n",
    "    \"\\\\xc3\\\\xb1\": \"ñ\",\n",
    "    \"\\\\xc3\\\\x9c\": \"Ü\",\n",
    "    \"\\\\xc3\\\\xbc\": \"ü\",\n",
    "    \"\\\\xe2\\\\x80\\\\x99\": \"'\",\n",
    "    \"\\\\xe2\\\\x80\\\\x9c\": \"\\\"\",\n",
    "    \"\\\\xe2\\\\x80\\\\x9d\": \"\\\"\"\n",
    "}\n",
    "\n",
    "# Function to replace special characters\n",
    "def fix_special_characters(text):\n",
    "    for wrong, correct in special_chars_mapping.items():\n",
    "        text = text.replace(wrong, correct)\n",
    "    return text\n",
    "\n",
    "# Process files to apply character mapping\n",
    "for file in os.listdir(cleaned_data_path):\n",
    "    file_path = os.path.join(cleaned_data_path, file)\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        content = f.read()\n",
    "        cleaned_content = fix_special_characters(content)\n",
    "    \n",
    "    # Save the updated file\n",
    "    final_file_path = os.path.join(final_output_path, file)\n",
    "    with open(final_file_path, \"w\", encoding=\"utf-8\") as out_file:\n",
    "        out_file.write(cleaned_content)\n",
    "    \n",
    "    print(f\"Processed special characters for: {file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 1: Removal of html codes, manage spaces issues, management of geo_metadata and special characters handling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Define the directories\n",
    "data_path = r\"C:\\\\Users\\\\larry\\\\OneDrive\\\\Documents\\\\GitHub\\\\project-aieng-interactive-travel-planner\\\\data\\\\landmarks\"#\\\\data_test\"\n",
    "output_path = r\"C:\\\\Users\\\\larry\\\\OneDrive\\\\Documents\\\\GitHub\\\\project-aieng-interactive-travel-planner\\\\data\\\\landmarks_cleanng\\\\step1_cleaned_data\"\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "# Step 1: Define Mapping\n",
    "special_chars_mapping = {\n",
    "    \"\\\\xc3\\\\xa1\": \"á\",\n",
    "    \"\\\\xc3\\\\xa9\": \"é\",\n",
    "    \"\\\\xc3\\\\xad\": \"í\",\n",
    "    \"\\\\xc3\\\\xb3\": \"ó\",\n",
    "    \"\\\\xc3\\\\xba\": \"ú\",\n",
    "    \"\\\\xc3\\\\xb1\": \"ñ\",\n",
    "    \"\\\\xc3\\\\x9c\": \"Ü\",\n",
    "    \"\\\\xc3\\\\xbc\": \"ü\",\n",
    "    \"\\\\xe2\\\\x80\\\\x99\": \"'\",\n",
    "    \"\\\\xe2\\\\x80\\\\x9c\": \"\\\"\",\n",
    "    \"\\\\xe2\\\\x80\\\\x9d\": \"\\\"\"\n",
    "}\n",
    "\n",
    "# Step 2: Function for Fixing Special Characters\n",
    "def fix_special_characters(text):\n",
    "    for wrong, correct in special_chars_mapping.items():\n",
    "        text = text.replace(wrong, correct)\n",
    "    return text\n",
    "\n",
    "# Step 3: Clean Function\n",
    "def clean_html(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        raw_content = f.read()\n",
    "\n",
    "    # Parse with BeautifulSoup\n",
    "    soup = BeautifulSoup(raw_content, \"html.parser\")\n",
    "\n",
    "    # Insert space after <a> tags and remove duplicates\n",
    "    for a in soup.find_all(\"a\"):\n",
    "        a.insert_after(\" \")\n",
    "\n",
    "    # Extract text\n",
    "    paragraphs = soup.find_all(['h1', 'h2', 'p'])\n",
    "    cleaned_text = \"\\n\".join([para.get_text(\" \", strip=True) for para in paragraphs])\n",
    "    \n",
    "    # Apply character replacement\n",
    "    cleaned_text = fix_special_characters(cleaned_text)\n",
    "\n",
    "    # Extract links and validate\n",
    "    links = list(set([a['href'] for a in soup.find_all('a', href=True)]))\n",
    "\n",
    "    # Extract geo metadata\n",
    "    geo_metadata = extract_geo_metadata_loose(raw_content)\n",
    "\n",
    "    # Combine all data\n",
    "    return {\n",
    "        \"main_content\": cleaned_text,\n",
    "        \"relevant_links\": links,\n",
    "        \"geo_metadata\": geo_metadata\n",
    "    }\n",
    "\n",
    "# Apply to All Files\n",
    "for file in os.listdir(data_path):\n",
    "    file_path = os.path.join(data_path, file)\n",
    "    cleaned_data = clean_html(file_path)\n",
    "    output_file_path = os.path.join(output_path, file)\n",
    "\n",
    "    with open(output_file_path, \"w\", encoding=\"utf-8\") as out_file:\n",
    "        json.dump(cleaned_data, out_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Processed: {file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 2: Removal of duplicate and broken links**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "\n",
    "# Define directories\n",
    "cleaned_data_path = r\"C:\\\\Users\\\\larry\\\\OneDrive\\\\Documents\\\\GitHub\\\\project-aieng-interactive-travel-planner\\\\data\\\\landmarks_cleanng\\\\step1_cleaned_data\"#cleaned_data\"\n",
    "final_data_path = r\"C:\\\\Users\\\\larry\\\\OneDrive\\\\Documents\\\\GitHub\\\\project-aieng-interactive-travel-planner\\\\data\\\\landmarks_cleanng\\\\step2_cleaned_data\"#final_data\"\n",
    "\n",
    "# Create the final output directory if it doesn't exist\n",
    "if not os.path.exists(final_data_path):\n",
    "    os.makedirs(final_data_path)\n",
    "\n",
    "# Function to validate and deduplicate links\n",
    "def process_relevant_links(links):\n",
    "    \"\"\"\n",
    "    Validates and deduplicates a list of relevant links.\n",
    "    Only keeps unique, valid links that start with \"http\".\n",
    "    \"\"\"\n",
    "    # Filter for HTTP(S) links\n",
    "    http_links = [link for link in links if link.startswith(\"http\")]\n",
    "    \n",
    "    # Deduplicate links\n",
    "    unique_links = list(set(http_links))\n",
    "    \n",
    "    # Validate links\n",
    "    valid_links = []\n",
    "    for link in unique_links:\n",
    "        try:\n",
    "            response = requests.head(link, timeout=5)\n",
    "            if response.status_code == 200:\n",
    "                valid_links.append(link)\n",
    "        except requests.RequestException:\n",
    "            continue\n",
    "\n",
    "    return valid_links\n",
    "\n",
    "# Process each file in the cleaned_data directory\n",
    "for file_name in os.listdir(cleaned_data_path):\n",
    "    file_path = os.path.join(cleaned_data_path, file_name)\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Process relevant links if they exist\n",
    "    if \"relevant_links\" in data:\n",
    "        data[\"relevant_links\"] = process_relevant_links(data[\"relevant_links\"])\n",
    "\n",
    "    # Save the updated data into the final_data directory\n",
    "    final_file_path = os.path.join(final_data_path, file_name)\n",
    "    with open(final_file_path, \"w\", encoding=\"utf-8\") as final_file:\n",
    "        json.dump(data, final_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Processed and saved: {file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 3: Removal of links references and not relevant links**\n",
    "Note: Removal of link references does not works within this code. LWD 06Feb25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Define input and output directories\n",
    "final_data_path = r\"C:\\\\Users\\\\larry\\\\OneDrive\\\\Documents\\\\GitHub\\\\project-aieng-interactive-travel-planner\\\\data\\\\landmarks_cleanng\\\\step2_cleaned_data\"#final_data\"\n",
    "final_cleaned_data_path = r\"C:\\\\Users\\\\larry\\\\OneDrive\\\\Documents\\\\GitHub\\\\project-aieng-interactive-travel-planner\\\\data\\\\landmarks_cleanng\\\\step3_cleaned_data\"\n",
    "\n",
    "# Create final cleaned output directory if it doesn't exist\n",
    "if not os.path.exists(final_cleaned_data_path):\n",
    "    os.makedirs(final_cleaned_data_path)\n",
    "\n",
    "# Define trusted sources for relevant links\n",
    "TRUSTED_DOMAINS = [\n",
    "    \"wikipedia.org\",\n",
    "    \"wikivoyage.org\",\n",
    "    \"discoverpuertorico.com\",\n",
    "    \"travel.usnews.com\",\n",
    "    \"tripadvisor.com\",\n",
    "    \"skyscanner.com\",\n",
    "    \"lonelyplanet.com\",\n",
    "    \"puertorico.com\"\n",
    "]\n",
    "\n",
    "# Function to filter relevant links\n",
    "def filter_relevant_links(links):\n",
    "    filtered_links = []\n",
    "    for link in links:\n",
    "        if any(domain in link for domain in TRUSTED_DOMAINS):\n",
    "            filtered_links.append(link)\n",
    "    return list(set(filtered_links))  # Remove duplicates\n",
    "\n",
    "# Function to remove citation numbers like \"[1]\" from text\n",
    "def remove_citation_numbers(text):\n",
    "    return re.sub(r\"\\[\\d+\\]\", \"\", text)\n",
    "\n",
    "# Process each file in the final_data directory\n",
    "for file_name in os.listdir(final_data_path):\n",
    "    file_path = os.path.join(final_data_path, file_name)\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Clean relevant links\n",
    "    if \"relevant_links\" in data:\n",
    "        data[\"relevant_links\"] = filter_relevant_links(data[\"relevant_links\"])\n",
    "\n",
    "    # Clean main content from citation numbers\n",
    "    if \"main_content\" in data:\n",
    "        data[\"main_content\"] = remove_citation_numbers(data[\"main_content\"])\n",
    "\n",
    "    # Save the cleaned file to final_cleaned_data directory\n",
    "    output_file_path = os.path.join(final_cleaned_data_path, file_name)\n",
    "    with open(output_file_path, \"w\", encoding=\"utf-8\") as output_file:\n",
    "        json.dump(data, output_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Final cleaned and saved: {file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Step 4: Removal of links references**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Define directories\n",
    "cleaned_data_path = r\"C:\\\\Users\\\\larry\\\\OneDrive\\\\Documents\\\\GitHub\\\\project-aieng-interactive-travel-planner\\\\data\\\\landmarks_cleanng\\\\step3_cleaned_data\"\n",
    "final_data_path = r\"C:\\\\Users\\\\larry\\\\OneDrive\\\\Documents\\\\GitHub\\\\project-aieng-interactive-travel-planner\\\\data\\\\landmarks_cleanng\\\\final_step_cleaned_data\"\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "if not os.path.exists(final_data_path):\n",
    "    os.makedirs(final_data_path)\n",
    "\n",
    "# Function to clean the relevant_links list\n",
    "def clean_relevant_links(links):\n",
    "    unique_links = list(set(links))  # Remove duplicates\n",
    "    return unique_links\n",
    "\n",
    "# Function to remove reference numbers like [3], [15], etc.\n",
    "def remove_reference_numbers(text):\n",
    "    return re.sub(r\"\\[ \\d+\\ ]\", \"\", text)\n",
    "\n",
    "# Process each file in cleaned_data\n",
    "for file in os.listdir(cleaned_data_path):\n",
    "    file_path = os.path.join(cleaned_data_path, file)\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        content = json.load(f)\n",
    "\n",
    "    # Clean the relevant_links\n",
    "    if \"relevant_links\" in content:\n",
    "        content[\"relevant_links\"] = clean_relevant_links(content[\"relevant_links\"])\n",
    "\n",
    "    # Remove reference numbers from main content\n",
    "    if \"main_content\" in content:\n",
    "        content[\"main_content\"] = remove_reference_numbers(content[\"main_content\"])\n",
    "\n",
    "    # Save the final cleaned content\n",
    "    output_file_path = os.path.join(final_data_path, file)\n",
    "    with open(output_file_path, \"w\", encoding=\"utf-8\") as out_file:\n",
    "        json.dump(content, out_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Final cleaned file saved: {file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Define paths\n",
    "raw_data_path = r\"C:\\Users\\larry\\OneDrive\\Documents\\GitHub\\project-aieng-interactive-travel-planner\\data\\landmarks\\test\"\n",
    "cleaned_data_path = r\"C:\\Users\\larry\\OneDrive\\Documents\\GitHub\\project-aieng-interactive-travel-planner\\data\\landmarks_cleanng\\final_step_cleaned_data\"                     \n",
    "output_path = r\"C:\\Users\\larry\\OneDrive\\Documents\\GitHub\\project-aieng-interactive-travel-planner\\data\\landmarks_cleanng\\10Feb25_step01_title_cat_geo_clean\"\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "# Keyword-based category mapping\n",
    "category_keywords = {\n",
    "    \"Extreme Adventure\": [\"zipline\", \"canopy\", \"extreme sports\", \"rock climbing\", \"caving\", \"spelunking\", \"skydiving\", \"bungee jumping\", \"off-road\", \"ATV\"],\n",
    "    \"Soft Adventure\": [\"hiking\", \"kayaking\", \"snorkeling\", \"paddleboarding\", \"scuba diving\", \"tubing\", \"water sports\"],\n",
    "    \"Outdoor\": [\"nature\", \"park\", \"trail\", \"scenic\", \"river\", \"camping\", \"wildlife\", \"birdwatching\", \"cave\", \"natural reserve\"],\n",
    "    \"Beach\": [\"beach\", \"sand\", \"ocean\", \"coast\", \"surfing\", \"snorkeling\", \"seaside\"],\n",
    "    \"Culture\": [\"museum\", \"history\", \"heritage\", \"colonial\", \"art\", \"landmark\", \"architecture\"],\n",
    "    \"Food\": [\"food\", \"cuisine\", \"restaurant\", \"culinary\", \"gastronomy\", \"local dishes\"],\n",
    "    \"Nightlife\": [\"nightlife\", \"bar\", \"party\", \"music\", \"club\", \"cocktail\", \"dance\"],\n",
    "    \"Shopping\": [\"shopping\", \"mall\", \"store\", \"boutique\", \"souvenirs\", \"market\"],\n",
    "    \"Wildlife\": [\"animals\", \"zoo\", \"wildlife\", \"birdwatching\", \"conservation\"],\n",
    "    \"Family-Friendly\": [\"family\", \"kids\", \"playground\", \"amusement\", \"theme park\", \"activities\"],\n",
    "    \"Religion\": [\"church\", \"cathedral\", \"parish\", \"temple\", \"sanctuary\", \"basilica\", \"shrine\", \"holy\", \"worship\"],\n",
    "    \"Wellness & Relaxation\": [\"spa\", \"wellness\", \"yoga\", \"retreat\", \"meditation\", \"hot springs\", \"thermal baths\", \"relaxation\", \"peaceful\"],\n",
    "    \"Luxury & Resorts\": [\"resort\", \"luxury\", \"boutique hotel\", \"exclusive\", \"private beach\", \"high-end\"],\n",
    "    \"Festivals & Events\": [\"festival\", \"event\", \"parade\", \"local traditions\", \"carnival\", \"concert\", \"Puerto Rican festival\"]\n",
    "}\n",
    "\n",
    "# Function to extract coordinates\n",
    "def extract_coordinates(text):\n",
    "    coordinate_pattern = re.compile(r'(-?\\d+\\.\\d+);\\s*(-?\\d+\\.\\d+)')\n",
    "    matches = coordinate_pattern.findall(text)\n",
    "    if matches:\n",
    "        lat, lon = float(matches[0][0]), float(matches[0][1])\n",
    "        return [lat, lon]\n",
    "    return None\n",
    "\n",
    "# Function to remove pronunciation sections (generic patterns within parentheses)\n",
    "def remove_pronunciation(text):\n",
    "    # Define the regex pattern to match different pronunciation formats\n",
    "    pronunciation_pattern = re.compile(\n",
    "        r\"\\(\\s*(US:.*?|UK:.*?|Spanish:.*?|US :.*?|UK :.*?|Spanish :.*?|.*?pronunciation:.*?|/.*?/.*?)\\s*\\)\",  # Include \"pronunciation:\" explicitly\n",
    "        flags=re.DOTALL\n",
    "    )\n",
    "    # Remove all matching patterns\n",
    "    return re.sub(pronunciation_pattern, \"\", text)\n",
    "\n",
    "# Function to assign categories based on keywords\n",
    "def assign_categories(text):\n",
    "    assigned_categories = []\n",
    "    text_lower = text.lower()\n",
    "    for category, keywords in category_keywords.items():\n",
    "        if any(keyword in text_lower for keyword in keywords):\n",
    "            assigned_categories.append(category)\n",
    "    return list(set(assigned_categories))  # Ensure unique categories\n",
    "\n",
    "# Process all files\n",
    "for file_name in os.listdir(cleaned_data_path):\n",
    "    raw_file_path = os.path.join(raw_data_path, file_name)\n",
    "    cleaned_file_path = os.path.join(cleaned_data_path, file_name)\n",
    "    \n",
    "    if not os.path.exists(raw_file_path):\n",
    "        print(f\"Skipping {file_name}: Raw file not found.\")\n",
    "        continue\n",
    "\n",
    "    # Load raw and cleaned data\n",
    "    with open(raw_file_path, \"r\", encoding=\"utf-8\") as raw_file:\n",
    "        raw_content = raw_file.read()\n",
    "\n",
    "    with open(cleaned_file_path, \"r\", encoding=\"utf-8\") as cleaned_file:\n",
    "        cleaned_data = json.load(cleaned_file)\n",
    "\n",
    "    # Extract title (use file name as fallback)\n",
    "    title_match = re.search(r\"<h1>(.*?)</h1>\", raw_content)\n",
    "    title = title_match.group(1).strip() if title_match else file_name.replace(\".txt\", \"\")\n",
    "\n",
    "    # Extract coordinates\n",
    "    coordinates = extract_coordinates(raw_content)\n",
    "    \n",
    "    # Assign categories\n",
    "    categories = assign_categories(cleaned_data[\"main_content\"])\n",
    "\n",
    "    # Remove pronunciation details\n",
    "    cleaned_data[\"main_content\"] = remove_pronunciation(cleaned_data[\"main_content\"])\n",
    "\n",
    "    # Update geo_metadata\n",
    "    cleaned_data[\"geo_metadata\"] = {\n",
    "        \"coordinates\": coordinates if coordinates else cleaned_data[\"geo_metadata\"].get(\"coordinates\", \"Not Found\"),\n",
    "        \"title\": title,\n",
    "        \"type\": \"Point\"\n",
    "    }\n",
    "\n",
    "    # Add categories\n",
    "    cleaned_data[\"categories\"] = categories\n",
    "\n",
    "    # Save updated JSON\n",
    "    output_file_path = os.path.join(output_path, file_name)\n",
    "    with open(output_file_path, \"w\", encoding=\"utf-8\") as out_file:\n",
    "        json.dump(cleaned_data, out_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Updated and saved: {file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sure, I can help you integrate the provided code into your existing code. Here is the updated code with the new functions and logic included:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Define paths\n",
    "raw_data_path = r\"C:\\Users\\larry\\OneDrive\\Documents\\GitHub\\project-aieng-interactive-travel-planner\\data\\landmarks\\test\"\n",
    "cleaned_data_path = r\"C:\\Users\\larry\\OneDrive\\Documents\\GitHub\\project-aieng-interactive-travel-planner\\data\\landmarks_cleanng\\final_step_cleaned_data\"                     \n",
    "output_path = r\"C:\\Users\\larry\\OneDrive\\Documents\\GitHub\\project-aieng-interactive-travel-planner\\data\\landmarks_cleanng\\10Feb25_step01_title_cat_geo_clean\"\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "# Keyword-based category mapping\n",
    "category_keywords = {\n",
    "    \"Extreme Adventure\": [\"zipline\", \"canopy\", \"extreme sports\", \"rock climbing\", \"caving\", \"spelunking\", \"skydiving\", \"bungee jumping\", \"off-road\", \"ATV\"],\n",
    "    \"Soft Adventure\": [\"hiking\", \"kayaking\", \"snorkeling\", \"paddleboarding\", \"scuba diving\", \"tubing\", \"water sports\"],\n",
    "    \"Outdoor\": [\"nature\", \"park\", \"trail\", \"scenic\", \"river\", \"camping\", \"wildlife\", \"birdwatching\", \"cave\", \"natural reserve\"],\n",
    "    \"Beach\": [\"beach\", \"sand\", \"ocean\", \"coast\", \"surfing\", \"snorkeling\", \"seaside\"],\n",
    "    \"Culture\": [\"museum\", \"history\", \"heritage\", \"colonial\", \"art\", \"landmark\", \"architecture\"],\n",
    "    \"Food\": [\"food\", \"cuisine\", \"restaurant\", \"culinary\", \"gastronomy\", \"local dishes\"],\n",
    "    \"Nightlife\": [\"nightlife\", \"bar\", \"party\", \"music\", \"club\", \"cocktail\", \"dance\"],\n",
    "    \"Shopping\": [\"shopping\", \"mall\", \"store\", \"boutique\", \"souvenirs\", \"market\"],\n",
    "    \"Wildlife\": [\"animals\", \"zoo\", \"wildlife\", \"birdwatching\", \"conservation\"],\n",
    "    \"Family-Friendly\": [\"family\", \"kids\", \"playground\", \"amusement\", \"theme park\", \"activities\"],\n",
    "    \"Religion\": [\"church\", \"cathedral\", \"parish\", \"temple\", \"sanctuary\", \"basilica\", \"shrine\", \"holy\", \"worship\"],\n",
    "    \"Wellness & Relaxation\": [\"spa\", \"wellness\", \"yoga\", \"retreat\", \"meditation\", \"hot springs\", \"thermal baths\", \"relaxation\", \"peaceful\"],\n",
    "    \"Luxury & Resorts\": [\"resort\", \"luxury\", \"boutique hotel\", \"exclusive\", \"private beach\", \"high-end\"],\n",
    "    \"Festivals & Events\": [\"festival\", \"event\", \"parade\", \"local traditions\", \"carnival\", \"concert\", \"Puerto Rican festival\"]\n",
    "}\n",
    "\n",
    "# Function to extract coordinates\n",
    "def extract_coordinates(text):\n",
    "    coordinate_pattern = re.compile(r'(-?\\d+\\.\\d+);\\s*(-?\\d+\\.\\d+)')\n",
    "    matches = coordinate_pattern.findall(text)\n",
    "    if matches:\n",
    "        lat, lon = float(matches[0][0]), float(matches[0][1])\n",
    "        return [lat, lon]\n",
    "    return None\n",
    "\n",
    "# Function to remove pronunciation sections (generic patterns within parentheses)\n",
    "def remove_pronunciation(text):\n",
    "    # Define the regex pattern to match different pronunciation formats\n",
    "    pronunciation_pattern = re.compile(\n",
    "        r\"\\(\\s*(US:.*?|UK:.*?|Spanish:.*?|US :.*?|UK :.*?|Spanish :.*?|.*?pronunciation:.*?|/.*?/.*?)\\s*\\)\",  # Include \"pronunciation:\" explicitly\n",
    "        flags=re.DOTALL\n",
    "    )\n",
    "    # Remove all matching patterns\n",
    "    return re.sub(pronunciation_pattern, \"\", text)\n",
    "\n",
    "# Function to assign categories based on keywords\n",
    "def assign_categories(text):\n",
    "    assigned_categories = []\n",
    "    text_lower = text.lower()\n",
    "    for category, keywords in category_keywords.items():\n",
    "        if any(keyword in text_lower for keyword in keywords):\n",
    "            assigned_categories.append(category)\n",
    "    return list(set(assigned_categories))  # Ensure unique categories\n",
    "\n",
    "# Function to update geo_metadata and categories in cleaned files\n",
    "def update_cleaned_file(cleaned_data, title, coordinates, categories):\n",
    "    # Validate and update geo_metadata\n",
    "    cleaned_data[\"geo_metadata\"] = {\n",
    "        \"coordinates\": coordinates,\n",
    "        \"title\": cleaned_data[\"geo_metadata\"].get(\"title\", title) if \"geo_metadata\" in cleaned_data else title,\n",
    "        \"type\": \"Point\"\n",
    "    }\n",
    "\n",
    "    # Add categories\n",
    "    cleaned_data[\"categories\"] = categories\n",
    "\n",
    "    return cleaned_data\n",
    "\n",
    "# Function to clean and update main content\n",
    "def update_main_content(cleaned_data):\n",
    "    # Remove pronunciation sections if available\n",
    "    if \"main_content\" in cleaned_data:\n",
    "        cleaned_data[\"main_content\"] = remove_pronunciation(cleaned_data[\"main_content\"])\n",
    "    return cleaned_data\n",
    "\n",
    "# Process files and handle exceptions\n",
    "for file in os.listdir(cleaned_data_path):\n",
    "    try:\n",
    "        cleaned_file_path = os.path.join(cleaned_data_path, file)\n",
    "        raw_file_path = os.path.join(raw_data_path, file)\n",
    "\n",
    "        # Load cleaned data\n",
    "        with open(cleaned_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            cleaned_data = json.load(f)\n",
    "\n",
    "        # Extract title, coordinates, and categories from raw data\n",
    "        with open(raw_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            raw_content = f.read()\n",
    "        title = re.search(r\"<h1>(.*?)</h1>\", raw_content).group(1).strip() if re.search(r\"<h1>(.*?)</h1>\", raw_content) else file.replace(\".txt\", \"\")\n",
    "        coordinates = extract_coordinates(raw_content)\n",
    "        categories = assign_categories(raw_content)\n",
    "\n",
    "        # Update cleaned data\n",
    "        cleaned_data = update_cleaned_file(cleaned_data, title, coordinates, categories)\n",
    "        cleaned_data = update_main_content(cleaned_data)\n",
    "\n",
    "        # Save updated cleaned data\n",
    "        output_file_path = os.path.join(output_path, file)\n",
    "        with open(output_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(cleaned_data, f, indent=4, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"Updated and saved: {file}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "This code integrates the new functions `update_cleaned_file` and `update_main_content` into the existing processing loop. It ensures that the cleaned data is updated with the title, coordinates, and categories, and that the main content is cleaned by removing pronunciation sections. The updated cleaned data is then saved to the specified output path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Define paths\n",
    "raw_data_path = r\"C:\\Users\\larry\\OneDrive\\Documents\\GitHub\\project-aieng-interactive-travel-planner\\data\\landmarks\\test\"\n",
    "cleaned_data_path = r\"C:\\Users\\larry\\OneDrive\\Documents\\GitHub\\project-aieng-interactive-travel-planner\\data\\landmarks_cleanng\\final_step_cleaned_data\"                     \n",
    "output_path = r\"C:\\Users\\larry\\OneDrive\\Documents\\GitHub\\project-aieng-interactive-travel-planner\\data\\landmarks_cleanng\\10Feb25a_step01_title_cat_geo_clean\"\n",
    "municipality_data_path = r\"C:\\Users\\larry\\OneDrive\\Documents\\GitHub\\project-aieng-interactive-travel-planner\\data\\municipalities_cleaning\\step3_final_cleaned_data\"\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "# Keyword-based category mapping\n",
    "category_keywords = {\n",
    "    \"Extreme Adventure\": [\"zipline\", \"canopy\", \"bike\", \"rappelling\", \"mountain bike\", \"paragliding\", \"skate\", \"skating\", \"extreme sports\", \"rock climbing\", \"caving\", \"spelunking\", \"skydiving\", \"bungee jumping\", \"off-road\", \"ATV\"],\n",
    "    \"Soft Adventure\": [\"hiking\", \"kayaking\", \"snorkeling\", \"paddleboarding\", \"scuba diving\", \"tubing\", \"water sports\"],\n",
    "    \"Outdoor\": [\"nature\", \"park\", \"trail\", \"scenic\", \"river\", \"camping\", \"wildlife\", \"birdwatching\", \"cave\", \"natural reserve\"],\n",
    "    \"Beach\": [\"beach\", \"sand\", \"ocean\", \"coast\", \"surfing\", \"snorkeling\", \"seaside\"],\n",
    "    \"Culture\": [\"museum\", \"history\", \"heritage\", \"colonial\", \"art\", \"landmark\", \"architecture\"],\n",
    "    \"Food\": [\"food\", \"cuisine\", \"restaurant\", \"culinary\", \"gastronomy\", \"local dishes\"],\n",
    "    \"Nightlife\": [\"nightlife\", \"bar\", \"party\", \"music\", \"club\", \"cocktail\", \"dance\"],\n",
    "    \"Shopping\": [\"shopping\", \"mall\", \"store\", \"boutique\", \"souvenirs\", \"market\"],\n",
    "    \"Wildlife\": [\"animals\", \"zoo\", \"wildlife\", \"birdwatching\", \"conservation\"],\n",
    "    \"Family-Friendly\": [\"family\", \"kids\", \"playground\", \"amusement\", \"theme park\", \"activities\"],\n",
    "    \"Religion\": [\"church\", \"cathedral\", \"parish\", \"temple\", \"sanctuary\", \"basilica\", \"shrine\", \"holy\", \"worship\"],\n",
    "    \"Wellness & Relaxation\": [\"spa\", \"wellness\", \"yoga\", \"retreat\", \"meditation\", \"hot springs\", \"thermal baths\", \"relaxation\", \"peaceful\"],\n",
    "    \"Luxury & Resorts\": [\"resort\", \"luxury\", \"boutique hotel\", \"exclusive\", \"private beach\", \"high-end\"],\n",
    "    \"Festivals & Events\": [\"festival\", \"event\", \"parade\", \"local traditions\", \"carnival\", \"concert\", \"Puerto Rican festival\"]\n",
    "}\n",
    "\n",
    "# Function to extract coordinates\n",
    "def extract_coordinates(text):\n",
    "    coordinate_pattern = re.compile(r'(-?\\d+\\.\\d+);\\s*(-?\\d+\\.\\d+)')\n",
    "    matches = coordinate_pattern.findall(text)\n",
    "    if matches:\n",
    "        lat, lon = float(matches[0][0]), float(matches[0][1])\n",
    "        return [lat, lon]\n",
    "    return None\n",
    "\n",
    "# Function to remove pronunciation sections (generic patterns within parentheses)\n",
    "def remove_pronunciation(text):\n",
    "    # Define the regex pattern to match different pronunciation formats\n",
    "    pronunciation_pattern = re.compile(\n",
    "        r\"\\(\\s*(US:.*?|UK:.*?|Spanish:.*?|US :.*?|UK :.*?|Spanish :.*?|.*?pronunciation:.*?|/.*?/.*?)\\s*\\)\",  # Include \"pronunciation:\" explicitly\n",
    "        flags=re.DOTALL\n",
    "    )\n",
    "    # Remove all matching patterns\n",
    "    return re.sub(pronunciation_pattern, \"\", text)\n",
    "\n",
    "# Function to assign categories based on keywords\n",
    "def assign_categories(text):\n",
    "    assigned_categories = []\n",
    "    text_lower = text.lower()\n",
    "    for category, keywords in category_keywords.items():\n",
    "        if any(keyword in text_lower for keyword in keywords):\n",
    "            assigned_categories.append(category)\n",
    "    return list(set(assigned_categories))  # Ensure unique categories\n",
    "\n",
    "# Function to update geo_metadata and categories in cleaned files\n",
    "def update_cleaned_file(cleaned_data, title, coordinates, categories):\n",
    "    # Validate and update geo_metadata\n",
    "    cleaned_data[\"geo_metadata\"] = {\n",
    "        \"coordinates\": coordinates if coordinates else cleaned_data[\"geo_metadata\"].get(\"coordinates\", \"Not Found\"),\n",
    "        \"title\": cleaned_data[\"geo_metadata\"].get(\"title\", title) if \"geo_metadata\" in cleaned_data else title,\n",
    "        \"type\": \"Point\"\n",
    "    }\n",
    "\n",
    "    # Add categories\n",
    "    cleaned_data[\"categories\"] = categories\n",
    "\n",
    "    return cleaned_data\n",
    "\n",
    "# Function to clean and update main content\n",
    "def update_main_content(cleaned_data):\n",
    "    # Remove pronunciation sections if available\n",
    "    if \"main_content\" in cleaned_data:\n",
    "        cleaned_data[\"main_content\"] = remove_pronunciation(cleaned_data[\"main_content\"])\n",
    "    return cleaned_data\n",
    "\n",
    "# Process files and handle exceptions\n",
    "files_without_coordinates = []\n",
    "for file in os.listdir(cleaned_data_path):\n",
    "    try:\n",
    "        cleaned_file_path = os.path.join(cleaned_data_path, file)\n",
    "        raw_file_path = os.path.join(raw_data_path, file)\n",
    "\n",
    "        # Load cleaned data\n",
    "        with open(cleaned_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            cleaned_data = json.load(f)\n",
    "\n",
    "        # Extract title, coordinates, and categories from raw data\n",
    "        with open(raw_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            raw_content = f.read()\n",
    "        title = re.search(r\"<h1>(.*?)</h1>\", raw_content).group(1).strip() if re.search(r\"<h1>(.*?)</h1>\", raw_content) else file.replace(\".txt\", \"\")\n",
    "        coordinates = extract_coordinates(raw_content)\n",
    "        categories = assign_categories(cleaned_data[\"main_content\"])\n",
    "\n",
    "        # Update cleaned data\n",
    "        cleaned_data = update_cleaned_file(cleaned_data, title, coordinates, categories)\n",
    "        cleaned_data = update_main_content(cleaned_data)\n",
    "\n",
    "        # Save updated cleaned data\n",
    "        output_file_path = os.path.join(output_path, file)\n",
    "        with open(output_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(cleaned_data, f, indent=4, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"Updated and saved: {file}\")\n",
    "\n",
    "        if not coordinates:\n",
    "            files_without_coordinates.append(file)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file}: {e}\")\n",
    "\n",
    "# List files without coordinates\n",
    "print(\"Files without coordinates:\")\n",
    "for file in files_without_coordinates:\n",
    "    print(file)\n",
    "\n",
    "# Assign coordinates based on municipality location\n",
    "for file in files_without_coordinates:\n",
    "    try:\n",
    "        cleaned_file_path = os.path.join(output_path, file)\n",
    "        with open(cleaned_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            cleaned_data = json.load(f)\n",
    "\n",
    "        # Assign coordinates based on municipality\n",
    "        for municipality_file in os.listdir(municipality_data_path):\n",
    "            municipality_name = municipality_file.replace(\".json\", \"\")\n",
    "            if municipality_name.lower() in cleaned_data[\"geo_metadata\"][\"title\"].lower():\n",
    "                municipality_file_path = os.path.join(municipality_data_path, municipality_file)\n",
    "                with open(municipality_file_path, \"r\", encoding=\"utf-8\") as mf:\n",
    "                    municipality_data = json.load(mf)\n",
    "                cleaned_data[\"geo_metadata\"][\"coordinates\"] = municipality_data[\"geo_metadata\"][\"coordinates\"]\n",
    "                break\n",
    "\n",
    "        # Save updated cleaned data with municipality coordinates\n",
    "        with open(cleaned_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(cleaned_data, f, indent=4, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"Assigned coordinates to: {file}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error assigning coordinates to file {file}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entiendo, vamos a realizar las siguientes modificaciones para solucionar los problemas:\n",
    "\n",
    "1. Asegurarnos de que la verificación de `geo_metadata` maneje correctamente los casos en los que no existe.\n",
    "2. Ajustar la asignación de categorías para que haga una coincidencia exacta de las palabras clave.\n",
    "3. Asegurarnos de que el sistema intente aplicar la información de los municipios cuando no se encuentren coordenadas.\n",
    "\n",
    "Aquí está el código actualizado:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Define paths\n",
    "raw_data_path = r\"C:\\Users\\larry\\OneDrive\\Documents\\GitHub\\project-aieng-interactive-travel-planner\\data\\landmarks\\test\"\n",
    "cleaned_data_path = r\"C:\\Users\\larry\\OneDrive\\Documents\\GitHub\\project-aieng-interactive-travel-planner\\data\\landmarks_cleanng\\final_step_cleaned_data\"                     \n",
    "output_path = r\"C:\\Users\\larry\\OneDrive\\Documents\\GitHub\\project-aieng-interactive-travel-planner\\data\\landmarks_cleanng\\10Feb25b_step01_title_cat_geo_clean\"\n",
    "municipality_data_path = r\"C:\\Users\\larry\\OneDrive\\Documents\\GitHub\\project-aieng-interactive-travel-planner\\data\\municipalities_cleaning\\step3_final_cleaned_data\"\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "# Keyword-based category mapping\n",
    "category_keywords = {\n",
    "    \"Extreme Adventure\": [\"zipline\", \"canopy\", \"rappelling\", \"mountain bike\", \"paragliding\", \"skate\", \"skating\", \"extreme sports\", \"rock climbing\", \"caving\", \"spelunking\", \"skydiving\", \"bungee jumping\", \"off-road\", \"ATV\"],\n",
    "    \"Soft Adventure\": [\"hiking\", \"kayaking\", \"bike\", \"snorkeling\", \"paddleboarding\", \"scuba diving\", \"tubing\", \"water sports\"],\n",
    "    \"Outdoor\": [\"nature\", \"park\", \"trail\", \"scenic\", \"river\", \"trails\", \"camping\", \"picnic\", \"wildlife\", \"birdwatching\", \"cave\", \"natural reserve\", \"outdoor\", \"forrest\"],\n",
    "    \"Beach\": [\"beach\", \"sand\", \"ocean\", \"coast\", \"surfing\", \"snorkeling\", \"seaside\"],\n",
    "    \"Culture\": [\"museum\", \"history\", \"heritage\", \"colonial\", \"art\", \"architecture\"],\n",
    "    \"Food\": [\"food\", \"cuisine\", \"restaurant\", \"culinary\", \"gastronomy\", \"local dishes\"],\n",
    "    \"Nightlife\": [\"nightlife\", \"bar\", \"party\", \"dance music\", \"night club\", \"cocktail\", \"dancing stage\", \"pub\", \"sport bar\"],\n",
    "    \"Shopping\": [\"shopping\", \"shop\", \"mall\", \"store\", \"boutique\", \"souvenirs\", \"market\"],\n",
    "    \"Wildlife\": [\"animals\", \"zoo\", \"wildlife\", \"birdwatching\", \"feeding\"],\n",
    "    \"Family-Friendly\": [\"family\", \"kids\", \"playground\", \"camping\", \"park\", \"amusement\", \"theme park\", \"picnic\", \"kite flying\", \"kites\"],\n",
    "    \"Religion\": [\"church\", \"cathedral\", \"parish\", \"temple\", \"sanctuary\", \"basilica\", \"shrine\", \"holy\", \"worship\"],\n",
    "    \"Wellness & Relaxation\": [\"spa\", \"wellness\", \"yoga\", \"retreat\", \"meditation\", \"hot springs\", \"thermal baths\", \"relaxation\", \"peaceful\"],\n",
    "    \"Luxury & Resorts\": [\"resort\", \"luxury\", \"boutique hotel\", \"exclusive\", \"private beach\", \"high-end\", \"hotel\", \"inn\", \"lodging\"],\n",
    "    \"Festivals & Events\": [\"festival\", \"event\", \"parade\", \"local traditions\", \"carnival\", \"concert\", \"Puerto Rican festival\"]\n",
    "}\n",
    "\n",
    "# Function to extract coordinates\n",
    "def extract_coordinates(text):\n",
    "    coordinate_pattern = re.compile(r'(-?\\d+\\.\\d+);\\s*(-?\\d+\\.\\d+)')\n",
    "    matches = coordinate_pattern.findall(text)\n",
    "    if matches:\n",
    "        lat, lon = float(matches[0][0]), float(matches[0][1])\n",
    "        return [lat, lon]\n",
    "    return None\n",
    "\n",
    "# Function to remove pronunciation sections (generic patterns within parentheses)\n",
    "def remove_pronunciation(text):\n",
    "    # Define the regex pattern to match different pronunciation formats\n",
    "    pronunciation_pattern = re.compile(\n",
    "        r\"\\(\\s*(US:.*?|UK:.*?|Spanish:.*?|US :.*?|UK :.*?|Spanish :.*?|.*?pronunciation:.*?|/.*?/.*?)\\s*\\)\",  # Include \"pronunciation:\" explicitly\n",
    "        flags=re.DOTALL\n",
    "    )\n",
    "    # Remove all matching patterns\n",
    "    return re.sub(pronunciation_pattern, \"\", text)\n",
    "\n",
    "# Function to assign categories based on keywords\n",
    "def assign_categories(text):\n",
    "    assigned_categories = []\n",
    "    text_lower = text.lower()\n",
    "    words = re.findall(r'\\b\\w+\\b', text_lower)  # Extract words for exact match\n",
    "    for category, keywords in category_keywords.items():\n",
    "        if any(keyword in words for keyword in keywords):\n",
    "            assigned_categories.append(category)\n",
    "    return list(set(assigned_categories))  # Ensure unique categories\n",
    "\n",
    "# Function to update geo_metadata and categories in cleaned files\n",
    "def update_cleaned_file(cleaned_data, title, coordinates, categories):\n",
    "    # Validate and update geo_metadata\n",
    "    if \"geo_metadata\" not in cleaned_data or cleaned_data[\"geo_metadata\"] is None:\n",
    "        cleaned_data[\"geo_metadata\"] = {}\n",
    "    cleaned_data[\"geo_metadata\"][\"coordinates\"] = coordinates if coordinates else cleaned_data[\"geo_metadata\"].get(\"coordinates\", \"Not Found\")\n",
    "    cleaned_data[\"geo_metadata\"][\"title\"] = cleaned_data[\"geo_metadata\"].get(\"title\", title)\n",
    "    cleaned_data[\"geo_metadata\"][\"type\"] = \"Point\"\n",
    "\n",
    "    # Add categories\n",
    "    cleaned_data[\"categories\"] = categories\n",
    "\n",
    "    return cleaned_data\n",
    "\n",
    "# Function to clean and update main content\n",
    "def update_main_content(cleaned_data):\n",
    "    # Remove pronunciation sections if available\n",
    "    if \"main_content\" in cleaned_data:\n",
    "        cleaned_data[\"main_content\"] = remove_pronunciation(cleaned_data[\"main_content\"])\n",
    "    return cleaned_data\n",
    "\n",
    "# Process files and handle exceptions\n",
    "files_without_coordinates = []\n",
    "for file in os.listdir(cleaned_data_path):\n",
    "    try:\n",
    "        cleaned_file_path = os.path.join(cleaned_data_path, file)\n",
    "        raw_file_path = os.path.join(raw_data_path, file)\n",
    "\n",
    "        # Load cleaned data\n",
    "        with open(cleaned_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            cleaned_data = json.load(f)\n",
    "\n",
    "        # Extract title, coordinates, and categories from raw data\n",
    "        with open(raw_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            raw_content = f.read()\n",
    "        title = re.search(r\"<h1>(.*?)</h1>\", raw_content).group(1).strip() if re.search(r\"<h1>(.*?)</h1>\", raw_content) else file.replace(\".txt\", \"\")\n",
    "        coordinates = extract_coordinates(raw_content)\n",
    "        categories = assign_categories(cleaned_data[\"main_content\"])\n",
    "\n",
    "        # Update cleaned data\n",
    "        cleaned_data = update_cleaned_file(cleaned_data, title, coordinates, categories)\n",
    "        cleaned_data = update_main_content(cleaned_data)\n",
    "\n",
    "        # Save updated cleaned data\n",
    "        output_file_path = os.path.join(output_path, file)\n",
    "        with open(output_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(cleaned_data, f, indent=4, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"Updated and saved: {file}\")\n",
    "\n",
    "        if not coordinates:\n",
    "            files_without_coordinates.append(file)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file}: {e}\")\n",
    "\n",
    "# List files without coordinates\n",
    "print(\"Files without coordinates:\")\n",
    "for file in files_without_coordinates:\n",
    "    print(file)\n",
    "\n",
    "# Assign coordinates based on municipality location\n",
    "for file in files_without_coordinates:\n",
    "    try:\n",
    "        cleaned_file_path = os.path.join(output_path, file)\n",
    "        with open(cleaned_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            cleaned_data = json.load(f)\n",
    "\n",
    "        # Assign coordinates based on municipality\n",
    "        for municipality_file in os.listdir(municipality_data_path):\n",
    "            municipality_name = municipality_file.replace(\".json\", \"\")\n",
    "            if municipality_name.lower() in cleaned_data[\"geo_metadata\"][\"title\"].lower():\n",
    "                municipality_file_path = os.path.join(municipality_data_path, municipality_file)\n",
    "                with open(municipality_file_path, \"r\", encoding=\"utf-8\") as mf:\n",
    "                    municipality_data = json.load(mf)\n",
    "                cleaned_data[\"geo_metadata\"][\"coordinates\"] = municipality_data[\"geo_metadata\"][\"coordinates\"]\n",
    "                break\n",
    "\n",
    "        # Save updated cleaned data with municipality coordinates\n",
    "        with open(cleaned_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(cleaned_data, f, indent=4, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"Assigned coordinates to: {file}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error assigning coordinates to file {file}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Este código realiza las siguientes modificaciones:\n",
    "1. Asegura que la verificación de `geo_metadata` maneje correctamente los casos en los que no existe.\n",
    "2. Ajusta la asignación de categorías para que haga una coincidencia exacta de las palabras clave.\n",
    "3. Asegura que el sistema intente aplicar la información de los municipios cuando no se encuentren coordenadas.\n",
    "\n",
    "Asegúrate de que los archivos de municipios en la carpeta `municipality_data_path` tengan la estructura correcta y contengan las coordenadas necesarias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Final Cleaning Step No. 1: Adding Title, geo info and Categories & removal of pronunciation section**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Define paths\n",
    "raw_data_path = r\"C:\\Users\\larry\\OneDrive\\Documents\\GitHub\\project-aieng-interactive-travel-planner\\data\\landmarks\"\n",
    "cleaned_data_path = r\"C:\\Users\\larry\\OneDrive\\Documents\\GitHub\\project-aieng-interactive-travel-planner\\data\\landmarks_cleaning\\final_step_cleaned_data\"                     \n",
    "output_path = r\"C:\\Users\\larry\\OneDrive\\Documents\\GitHub\\project-aieng-interactive-travel-planner\\data\\landmarks_cleaning\\10Feb25_final_step01_title_cat_geo_clean\"\n",
    "municipality_data_path = r\"C:\\Users\\larry\\OneDrive\\Documents\\GitHub\\project-aieng-interactive-travel-planner\\data\\municipalities_cleaning\\step3_final_cleaned_data\"\n",
    "no_coordinates_output_path = r\"C:\\Users\\larry\\OneDrive\\Documents\\GitHub\\project-aieng-interactive-travel-planner\\data\\landmarks_cleaning\\no_coordinates_records.json\"\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "# Keyword-based category mapping\n",
    "category_keywords = {\n",
    "    \"Extreme Adventure\": [\"zipline\", \"canopy\", \"rappelling\", \"mountain bike\", \"paragliding\", \"skate\", \"skating\", \"extreme sports\", \"rock climbing\", \"caving\", \"spelunking\", \"skydiving\", \"bungee jumping\", \"off-road\", \"ATV\"],\n",
    "    \"Soft Adventure\": [\"hiking\", \"kayaking\", \"bike\", \"snorkeling\", \"paddleboarding\", \"scuba diving\", \"tubing\", \"water sports\"],\n",
    "    \"Outdoor\": [\"nature\", \"park\", \"trail\", \"scenic\", \"river\", \"trails\", \"camping\", \"picnic\", \"wildlife\", \"birdwatching\", \"cave\", \"natural reserve\", \"outdoor\", \"forrest\"],\n",
    "    \"Beach\": [\"beach\", \"sand\", \"ocean\", \"coast\", \"surfing\", \"snorkeling\", \"seaside\"],\n",
    "    \"Culture\": [\"museum\", \"history\", \"heritage\", \"colonial\", \"art\", \"architecture\"],\n",
    "    \"Food\": [\"food\", \"cuisine\", \"restaurant\", \"culinary\", \"gastronomy\", \"local dishes\"],\n",
    "    \"Nightlife\": [\"nightlife\", \"bar\", \"party\", \"dance music\", \"night club\", \"cocktail\", \"dancing stage\", \"pub\", \"sport bar\"],\n",
    "    \"Shopping\": [\"shopping\", \"shop\", \"mall\", \"store\", \"boutique\", \"souvenirs\", \"market\"],\n",
    "    \"Wildlife\": [\"animals\", \"zoo\", \"wildlife\", \"birdwatching\", \"feeding\"],\n",
    "    \"Family-Friendly\": [\"family\", \"kids\", \"playground\", \"camping\", \"park\", \"amusement\", \"theme park\", \"picnic\", \"kite flying\", \"kites\"],\n",
    "    \"Religion\": [\"church\", \"cathedral\", \"parish\", \"temple\", \"sanctuary\", \"basilica\", \"shrine\", \"holy\", \"worship\"],\n",
    "    \"Wellness & Relaxation\": [\"spa\", \"wellness\", \"yoga\", \"retreat\", \"meditation\", \"hot springs\", \"thermal baths\", \"relaxation\", \"peaceful\"],\n",
    "    \"Luxury & Resorts\": [\"resort\", \"luxury\", \"boutique hotel\", \"exclusive\", \"private beach\", \"high-end\", \"hotel\", \"inn\", \"lodging\"],\n",
    "    \"Festivals & Events\": [\"festival\", \"event\", \"parade\", \"local traditions\", \"carnival\", \"concert\", \"Puerto Rican festival\"]\n",
    "}\n",
    "\n",
    "# Function to extract coordinates\n",
    "def extract_coordinates(text):\n",
    "    coordinate_pattern = re.compile(r'(-?\\d+\\.\\d+);\\s*(-?\\d+\\.\\d+)')\n",
    "    matches = coordinate_pattern.findall(text)\n",
    "    if matches:\n",
    "        lat, lon = float(matches[0][0]), float(matches[0][1])\n",
    "        return [lat, lon]\n",
    "    return None\n",
    "\n",
    "# Function to remove pronunciation sections (generic patterns within parentheses)\n",
    "def remove_pronunciation(text):\n",
    "    # Define the regex pattern to match different pronunciation formats\n",
    "    pronunciation_pattern = re.compile(\n",
    "        r\"\\(\\s*(US:.*?|UK:.*?|Spanish:.*?|US :.*?|UK :.*?|Spanish :.*?|.*?pronunciation:.*?|/.*?/.*?)\\s*\\)\",  # Include \"pronunciation:\" explicitly\n",
    "        flags=re.DOTALL\n",
    "    )\n",
    "    # Remove all matching patterns\n",
    "    return re.sub(pronunciation_pattern, \"\", text)\n",
    "\n",
    "# Function to assign categories based on keywords\n",
    "def assign_categories(text):\n",
    "    assigned_categories = []\n",
    "    text_lower = text.lower()\n",
    "    words = re.findall(r'\\b\\w+\\b', text_lower)  # Extract words for exact match\n",
    "    for category, keywords in category_keywords.items():\n",
    "        if any(keyword in words for keyword in keywords):\n",
    "            assigned_categories.append(category)\n",
    "    return list(set(assigned_categories))  # Ensure unique categories\n",
    "\n",
    "# Function to format title from filename\n",
    "def format_title(filename):\n",
    "    return ' '.join(word.capitalize() for word in filename.replace(\".txt\", \"\").replace(\"_\", \" \").split())\n",
    "\n",
    "# Function to update geo_metadata and categories in cleaned files\n",
    "def update_cleaned_file(cleaned_data, title, coordinates, categories):\n",
    "    # Validate and update geo_metadata\n",
    "    if \"geo_metadata\" not in cleaned_data or cleaned_data[\"geo_metadata\"] is None:\n",
    "        cleaned_data[\"geo_metadata\"] = {}\n",
    "    cleaned_data[\"geo_metadata\"][\"coordinates\"] = coordinates if coordinates else cleaned_data[\"geo_metadata\"].get(\"coordinates\", \"Not Found\")\n",
    "    cleaned_data[\"geo_metadata\"][\"title\"] = cleaned_data[\"geo_metadata\"].get(\"title\", title)\n",
    "    cleaned_data[\"geo_metadata\"][\"type\"] = \"Point\"\n",
    "\n",
    "    # Add categories\n",
    "    cleaned_data[\"categories\"] = categories\n",
    "\n",
    "    return cleaned_data\n",
    "\n",
    "# Function to clean and update main content\n",
    "def update_main_content(cleaned_data):\n",
    "    # Remove pronunciation sections if available\n",
    "    if \"main_content\" in cleaned_data:\n",
    "        cleaned_data[\"main_content\"] = remove_pronunciation(cleaned_data[\"main_content\"])\n",
    "    return cleaned_data\n",
    "\n",
    "# Process files and handle exceptions\n",
    "files_without_coordinates = []\n",
    "for file in os.listdir(cleaned_data_path):\n",
    "    try:\n",
    "        cleaned_file_path = os.path.join(cleaned_data_path, file)\n",
    "        raw_file_path = os.path.join(raw_data_path, file)\n",
    "\n",
    "        # Load cleaned data\n",
    "        with open(cleaned_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            cleaned_data = json.load(f)\n",
    "\n",
    "        # Extract title, coordinates, and categories from raw data\n",
    "        with open(raw_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            raw_content = f.read()\n",
    "        title = re.search(r\"<h1>(.*?)</h1>\", raw_content).group(1).strip() if re.search(r\"<h1>(.*?)</h1>\", raw_content) else format_title(file)\n",
    "        coordinates = extract_coordinates(raw_content)\n",
    "        categories = assign_categories(cleaned_data[\"main_content\"])\n",
    "\n",
    "        # Update cleaned data\n",
    "        cleaned_data = update_cleaned_file(cleaned_data, title, coordinates, categories)\n",
    "        cleaned_data = update_main_content(cleaned_data)\n",
    "\n",
    "        # Save updated cleaned data\n",
    "        output_file_path = os.path.join(output_path, file)\n",
    "        with open(output_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(cleaned_data, f, indent=4, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"Updated and saved: {file}\")\n",
    "\n",
    "        if not coordinates:\n",
    "            files_without_coordinates.append(file)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file}: {e}\")\n",
    "\n",
    "# List files without coordinates\n",
    "print(\"Files without coordinates:\")\n",
    "for file in files_without_coordinates:\n",
    "    print(file)\n",
    "\n",
    "# Assign coordinates based on municipality location\n",
    "for file in files_without_coordinates:\n",
    "    try:\n",
    "        cleaned_file_path = os.path.join(output_path, file)\n",
    "        with open(cleaned_file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            cleaned_data = json.load(f)\n",
    "\n",
    "        # Assign coordinates based on municipality\n",
    "        for municipality_file in os.listdir(municipality_data_path):\n",
    "            municipality_name = municipality_file.replace(\".json\", \"\")\n",
    "            if municipality_name.lower() in cleaned_data[\"geo_metadata\"][\"title\"].lower():\n",
    "                municipality_file_path = os.path.join(municipality_data_path, municipality_file)\n",
    "                with open(municipality_file_path, \"r\", encoding=\"utf-8\") as mf:\n",
    "                    municipality_data = json.load(mf)\n",
    "                cleaned_data[\"geo_metadata\"][\"coordinates\"] = municipality_data[\"geo_metadata\"][\"coordinates\"]\n",
    "                break\n",
    "\n",
    "        # Save updated cleaned data with municipality coordinates\n",
    "        with open(cleaned_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(cleaned_data, f, indent=4, ensure_ascii=False)\n",
    "        \n",
    "        print(f\"Assigned coordinates to: {file}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error assigning coordinates to file {file}: {e}\")\n",
    "\n",
    "# Save list of files without coordinates to a JSON file\n",
    "with open(no_coordinates_output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(files_without_coordinates, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "print(f\"List of files without coordinates saved to {no_coordinates_output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Final Cleaning Step No. 2: Update relevant links & Special Characters Removal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "\n",
    "# Define input and output directories\n",
    "final_data_path = r\"C:\\Users\\larry\\OneDrive\\Documents\\GitHub\\project-aieng-interactive-travel-planner\\data\\landmarks_cleaning\\10Feb25_final_step01_title_cat_geo_clean\"\n",
    "final_cleaned_data_path = r\"C:\\Users\\larry\\OneDrive\\Documents\\GitHub\\project-aieng-interactive-travel-planner\\data\\landmarks_cleaning\\10Feb25_final_step02_links_clean\"\n",
    "\n",
    "# Create final cleaned output directory if it doesn't exist\n",
    "if not os.path.exists(final_cleaned_data_path):\n",
    "    os.makedirs(final_cleaned_data_path)\n",
    "\n",
    "# Define whitelisted domains for relevant links\n",
    "WHITELISTED_DOMAINS = [\"en.wikipedia.org\", \n",
    "                       \"es.wikipedia.org\", \n",
    "                       \"en.wikivoyage.org\",  \n",
    "                       \"es.wikivoyage.org\", \n",
    "                       \"discoverpuertorico.com\", \n",
    "                       \"travel.usnews.com\", \n",
    "                       \"tripadvisor.com\", \n",
    "                       \"skyscanner.com\", \n",
    "                       \"lonelyplanet.com\", \n",
    "                       \"puertorico.com\",\n",
    "                       \"geohack.toolforge.org\", \n",
    "                       \"paralanaturaleza.org\", \n",
    "                       \"sanjuanpuertorico.com\", \n",
    "                       \"zeepuertorico.com\", \n",
    "                       \"weather-us.com\", \n",
    "                       \"estuario.org\", \n",
    "                       \"lonelyplanet.com\", \n",
    "                       \"toroverdepr.com\", \n",
    "                       \"distritot-mobile.com\", \n",
    "                       \"prcomiccon.com\", \n",
    "                       \"www.nps.gov\", \n",
    "                       \"prconvention.com\", \n",
    "                       \"nationalgeographic.com\", \n",
    "                       \"areciboweb.50megs.com\", \n",
    "                       \"flickr.com\", \n",
    "                       \"islaculebra.com\",\n",
    "                       ]\n",
    "# Function to filter relevant links based on whitelist\n",
    "def filter_relevant_links(links):\n",
    "    filtered_links = []\n",
    "    for link in links:\n",
    "        if any(domain in link for domain in WHITELISTED_DOMAINS) and \"web.archive.org\" not in link and \"www.gotopuertorico.com\" not in link and \"stats.wikimedia.org\" not in link:\n",
    "            filtered_links.append(link)\n",
    "    return list(set(filtered_links))  # Remove duplicates\n",
    "\n",
    "# Define Main Special Characters Mapping\n",
    "special_chars_mapping = {\n",
    "    \"\\\\xc3\\\\xa1\": \"á\",\n",
    "    \"\\\\xc3\\\\xa9\": \"é\",\n",
    "    \"\\\\xc3\\\\xad\": \"í\",\n",
    "    \"\\\\xc3\\\\xb3\": \"ó\",\n",
    "    \"\\\\xc3\\\\xba\": \"ú\",\n",
    "    \"\\\\xc3\\\\xb1\": \"ñ\",\n",
    "    \"\\\\xc3\\\\x9c\": \"Ü\",\n",
    "    \"\\\\xc3\\\\xbc\": \"ü\",\n",
    "    \"\\\\xe2\\\\x80\\\\x99\": \"'\",\n",
    "    \"\\\\xe2\\\\x80\\\\x9c\": \"\\\"\",\n",
    "    \"\\\\xe2\\\\x80\\\\x9d\": \"\\\"\",\n",
    "    \"\\\\xca\\\\x9du\\\\xca\\\\x9d\": \"ü\",\n",
    "    \"\\\\xcb\\\\x88\": \"'\",\n",
    "    \"\\\\xa\": \" \",\n",
    "    \"\\\\xc2\\\\xb0F\": \"°F\",\n",
    "    \"\\\\xc2\\\\xb0C\": \"°C\",\n",
    "    \"\\\\xc2\\\\xb0\": \"°\",\n",
    "    \"\\\\xe2\\\\x80\\\\x93\": \"-\"\n",
    "}\n",
    "\n",
    "# Function for Fixing Special Characters\n",
    "def fix_special_characters(text):\n",
    "    for wrong, correct in special_chars_mapping.items():\n",
    "        text = text.replace(wrong, correct)\n",
    "    return text\n",
    "\n",
    "# Function to ensure the relevant Wikipedia link is present\n",
    "def ensure_wikipedia_link(links, landmark_name):\n",
    "    # Generate the expected Wikipedia link\n",
    "    wikipedia_link = f\"https://en.wikipedia.org/wiki/{landmark_name}\"\n",
    "    \n",
    "    # Check if the link is already in the list, if not, add it\n",
    "    if wikipedia_link not in links:\n",
    "        links.append(wikipedia_link)\n",
    "    \n",
    "    return links\n",
    "\n",
    "# Process each file in the final_data directory\n",
    "for file_name in os.listdir(final_data_path):\n",
    "    file_path = os.path.join(final_data_path, file_name)\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Clean relevant links\n",
    "    if \"relevant_links\" in data:\n",
    "        data[\"relevant_links\"] = filter_relevant_links(data[\"relevant_links\"])\n",
    "\n",
    "    # Clean main content from Special Characters\n",
    "    if \"main_content\" in data:\n",
    "        data[\"main_content\"] = fix_special_characters(data[\"main_content\"])\n",
    "\n",
    "    # Ensure relevant links contain the Wikipedia link for the landmark\n",
    "    if \"relevant_links\" in data:\n",
    "        landmark_name = os.path.splitext(file_name)[0]  # Extract landmark name from the file name\n",
    "        data[\"relevant_links\"] = ensure_wikipedia_link(data[\"relevant_links\"], landmark_name)\n",
    "\n",
    "    # Save the cleaned file to final_cleaned_data directory\n",
    "    output_file_path = os.path.join(final_cleaned_data_path, file_name)\n",
    "    with open(output_file_path, \"w\", encoding=\"utf-8\") as output_file:\n",
    "        json.dump(data, output_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Final cleaned and saved: {file_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Final Cleaning Step No. 3: Fix Special Characters on Title**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# Define input and output directories\n",
    "final_data_path = r\"C:\\Users\\larry\\OneDrive\\Documents\\GitHub\\project-aieng-interactive-travel-planner\\data\\landmarks_cleaning\\10Feb25_final_step02_links_clean\"\n",
    "final_cleaned_data_path = r\"C:\\Users\\larry\\OneDrive\\Documents\\GitHub\\project-aieng-interactive-travel-planner\\data\\landmarks_cleaning\\11Feb25_final_step03_chars_title_clean\"\n",
    "\n",
    "# Create final cleaned output directory if it doesn't exist\n",
    "if not os.path.exists(final_cleaned_data_path):\n",
    "    os.makedirs(final_cleaned_data_path)\n",
    "\n",
    "# Define Main Special Characters Mapping\n",
    "special_chars_mapping = {\n",
    "    \"\\\\xc3\\\\xa1\": \"á\",\n",
    "    \"\\\\xc3\\\\xa9\": \"é\",\n",
    "    \"\\\\xc3\\\\xad\": \"í\",\n",
    "    \"\\\\xc3\\\\xb3\": \"ó\",\n",
    "    \"\\\\xc3\\\\xba\": \"ú\",\n",
    "    \"\\\\xc3\\\\xb1\": \"ñ\",\n",
    "    \"\\\\xc3\\\\x9c\": \"Ü\",\n",
    "    \"\\\\xc3\\\\xbc\": \"ü\",\n",
    "    \"\\\\xe2\\\\x80\\\\x99\": \"'\",\n",
    "    \"\\\\xe2\\\\x80\\\\x9c\": \"\\\"\",\n",
    "    \"\\\\xe2\\\\x80\\\\x9d\": \"\\\"\",\n",
    "    \"\\\\xca\\\\x9du\\\\xca\\\\x9d\": \"ü\",\n",
    "    \"\\\\xcb\\\\x88\": \"'\",\n",
    "    \"\\\\xa\": \" \",\n",
    "    \"\\\\xc2\\\\xb0F\": \"°F\",\n",
    "    \"\\\\xc2\\\\xb0C\": \"°C\",\n",
    "    \"\\\\xc2\\\\xb0\": \"°\",\n",
    "    \"\\\\xe2\\\\x80\\\\x93\": \"-\",\n",
    "    \"\\\\\\\\u0026amp;\": \"&\"\n",
    "}\n",
    "\n",
    "# Function for Fixing Special Characters\n",
    "def fix_special_characters(text):\n",
    "    for wrong, correct in special_chars_mapping.items():\n",
    "        text = text.replace(wrong, correct)\n",
    "    return text\n",
    "\n",
    "# Process each file in the final_data directory\n",
    "for file_name in os.listdir(final_data_path):\n",
    "    file_path = os.path.join(final_data_path, file_name)\n",
    "\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    # Fix special characters in geo_metadata title\n",
    "    if \"geo_metadata\" in data and \"title\" in data[\"geo_metadata\"]:\n",
    "        data[\"geo_metadata\"][\"title\"] = fix_special_characters(data[\"geo_metadata\"][\"title\"])\n",
    "\n",
    "    # Save the cleaned file to final_cleaned_data directory\n",
    "    output_file_path = os.path.join(final_cleaned_data_path, file_name)\n",
    "    with open(output_file_path, \"w\", encoding=\"utf-8\") as output_file:\n",
    "        json.dump(data, output_file, indent=4, ensure_ascii=False)\n",
    "\n",
    "    print(f\"Final cleaned and saved: {file_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
