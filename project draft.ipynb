{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total de files '.txt' en /data= 3963"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def count_files(directory):\n",
    "    count = 0\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if not file.endswith('.zip'):\n",
    "                count += 1\n",
    "    return count\n",
    "\n",
    "directory = 'data'\n",
    "file_count = count_files(directory)\n",
    "print(f'There are {file_count} files in the \"{directory}\" folder excluding .zip files.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desgloce de files dentro de cada folder\n",
    "821+821+1668+574+78=3962\n",
    "\n",
    "news_eng + news_espa + News40 + landmarks + municipios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def list_files_in_subdirectories(base_directory, subdirectories):\n",
    "    subdirectory_files = {}\n",
    "    \n",
    "    for subdirectory in subdirectories:\n",
    "        full_path = os.path.join(base_directory, subdirectory)\n",
    "        if os.path.exists(full_path):\n",
    "            files = []\n",
    "            for root, dirs, file_list in os.walk(full_path):\n",
    "                for file in file_list:\n",
    "                    if not file.endswith('.zip'):\n",
    "                        files.append(file)\n",
    "            file_details = [{'name': f, 'extension': os.path.splitext(f)[1]} for f in files]\n",
    "            subdirectory_files[subdirectory] = file_details\n",
    "        else:\n",
    "            subdirectory_files[subdirectory] = None\n",
    "    \n",
    "    return subdirectory_files\n",
    "\n",
    "base_directory = 'data'\n",
    "subdirectories = [\n",
    "    'elmundo_chunked_en_page1_15years',\n",
    "    'elmundo_chunked_es_page1_15years',\n",
    "    'elmundo_chunked_es_page1_40years',\n",
    "    'landmarks',\n",
    "    'municipalities'\n",
    "]\n",
    "\n",
    "subdirectory_files = list_files_in_subdirectories(base_directory, subdirectories)\n",
    "\n",
    "for subdirectory, files in subdirectory_files.items():\n",
    "    if files is not None:\n",
    "        print(f'Files in {subdirectory}:')\n",
    "        print(f'Total files: {len(files)}')\n",
    "        for file in files:\n",
    "            print(f'  - {file[\"name\"]} (Format: {file[\"extension\"]})')\n",
    "    else:\n",
    "        print(f'{subdirectory} does not exist.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Files Duplicados de news = 822\n",
    "\n",
    "Por el momento son los files duplicados de (data/elmundo_chunked_es_page1_15years) y (data/elmundo_chunked_es_page1_40years), que se repiten files en español o están vacios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import hashlib\n",
    "\n",
    "def get_file_hash(file_path):\n",
    "    hasher = hashlib.md5()\n",
    "    with open(file_path, 'rb') as f:\n",
    "        buf = f.read()\n",
    "        hasher.update(buf)\n",
    "    return hasher.hexdigest()\n",
    "\n",
    "def list_files_with_hashes(base_directory, subdirectories):\n",
    "    file_hashes = {}\n",
    "    \n",
    "    for subdirectory in subdirectories:\n",
    "        full_path = os.path.join(base_directory, subdirectory)\n",
    "        if os.path.exists(full_path):\n",
    "            for root, dirs, files in os.walk(full_path):\n",
    "                for file in files:\n",
    "                    if not file.endswith('.zip'):\n",
    "                        file_path = os.path.join(root, file)\n",
    "                        file_hash = get_file_hash(file_path)\n",
    "                        if file_hash in file_hashes:\n",
    "                            file_hashes[file_hash].append(file_path)\n",
    "                        else:\n",
    "                            file_hashes[file_hash] = [file_path]\n",
    "    \n",
    "    return file_hashes\n",
    "\n",
    "base_directory = 'data'\n",
    "subdirectories = [\n",
    "    'elmundo_chunked_en_page1_15years',\n",
    "    'elmundo_chunked_es_page1_15years',\n",
    "    'elmundo_chunked_es_page1_40years'\n",
    "]\n",
    "\n",
    "file_hashes = list_files_with_hashes(base_directory, subdirectories)\n",
    "\n",
    "duplicate_files = {hash: paths for hash, paths in file_hashes.items() if len(paths) > 1}\n",
    "\n",
    "print(f'Total identical files: {len(duplicate_files)}')\n",
    "for file_hash, paths in duplicate_files.items():\n",
    "    print(f'Hash: {file_hash}')\n",
    "    for path in paths:\n",
    "        print(f'  - {path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Data Repository for landmarks, municipalities, and news (3 el mundo chunked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "def read_files_in_directory(directory):\n",
    "    data = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if not file.endswith('.zip'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "                    data.append({'file_name': file, 'content': content})\n",
    "    return data\n",
    "\n",
    "def extract_landmarks_data(data):\n",
    "    # Placeholder for extracting landmarks data\n",
    "    landmarks = []\n",
    "    for item in data:\n",
    "        # Extract relevant details from item['content']\n",
    "        landmarks.append({'file_name': item['file_name'], 'details': item['content']})\n",
    "    return landmarks\n",
    "\n",
    "def extract_municipalities_data(data):\n",
    "    # Placeholder for extracting municipalities data\n",
    "    municipalities = []\n",
    "    for item in data:\n",
    "        # Extract relevant details from item['content']\n",
    "        municipalities.append({'file_name': item['file_name'], 'details': item['content']})\n",
    "    return municipalities\n",
    "\n",
    "def extract_news_data(data):\n",
    "    # Placeholder for extracting news data\n",
    "    news = []\n",
    "    for item in data:\n",
    "        # Extract relevant details from item['content']\n",
    "        news.append({'file_name': item['file_name'], 'details': item['content']})\n",
    "    return news\n",
    "\n",
    "base_directory = 'data'\n",
    "directories = {\n",
    "    'landmarks': 'landmarks',\n",
    "    'municipalities': 'municipalities',\n",
    "    'news': [\n",
    "        'elmundo_chunked_en_page1_15years',\n",
    "        'elmundo_chunked_es_page1_15years',\n",
    "        'elmundo_chunked_es_page1_40years'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Read and extract data from landmarks\n",
    "landmarks_data = read_files_in_directory(os.path.join(base_directory, directories['landmarks']))\n",
    "landmarks = extract_landmarks_data(landmarks_data)\n",
    "\n",
    "# Read and extract data from municipalities\n",
    "municipalities_data = read_files_in_directory(os.path.join(base_directory, directories['municipalities']))\n",
    "municipalities = extract_municipalities_data(municipalities_data)\n",
    "\n",
    "# Read and extract data from news\n",
    "news_data = []\n",
    "for news_dir in directories['news']:\n",
    "    news_data.extend(read_files_in_directory(os.path.join(base_directory, news_dir)))\n",
    "news = extract_news_data(news_data)\n",
    "\n",
    "# Combine all data into a single repository\n",
    "data_repository = {\n",
    "    'landmarks': landmarks,\n",
    "    'municipalities': municipalities,\n",
    "    'news': news\n",
    "}\n",
    "\n",
    "# Save the data repository to a JSON file\n",
    "with open('puerto_rico_data_repository.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(data_repository, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print('Data repository created successfully.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "puerto_rico_data_repository.json /// data repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Extract Key Details: **  Use NLP techniques to extract relevant information from each file.\n",
    "Clean and Enhance Text Data: Process and clean the text data to ensure it is suitable for retrieval and generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "# Download NLTK data files (only need to run once)\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "##Reading the files in the directory\n",
    "def read_files_in_directory(directory):\n",
    "    data = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if not file.endswith('.zip'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "                    data.append({'file_name': file, 'content': content})\n",
    "    return data\n",
    "\n",
    "## Cleaning the text\n",
    "def clean_text(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "    return ' '.join(filtered_text)\n",
    "\n",
    "\n",
    "##Extracting the data from the landmarks\n",
    "def extract_landmarks_data(data):\n",
    "    landmarks = []\n",
    "    for item in data:\n",
    "        content = clean_text(item['content'])\n",
    "        # Extract relevant details from content\n",
    "        summary = ' '.join(sent_tokenize(content)[:2])  # Simple summary: first 2 sentences\n",
    "        landmarks.append({'file_name': item['file_name'], 'summary': summary})\n",
    "    return landmarks\n",
    "\n",
    "##Extracting the data from the municipalities\n",
    "def extract_municipalities_data(data):\n",
    "    municipalities = []\n",
    "    for item in data:\n",
    "        content = clean_text(item['content'])\n",
    "        # Extract relevant details from content\n",
    "        summary = ' '.join(sent_tokenize(content)[:2])  # Simple summary: first 2 sentences\n",
    "        municipalities.append({'file_name': item['file_name'], 'summary': summary})\n",
    "    return municipalities\n",
    "\n",
    "##Extracting the data from the news\n",
    "def extract_news_data(data):\n",
    "    news = []\n",
    "    for item in data:\n",
    "        content = clean_text(item['content'])\n",
    "        # Extract relevant details from content\n",
    "        summary = ' '.join(sent_tokenize(content)[:2])  # Simple summary: first 2 sentences\n",
    "        news.append({'file_name': item['file_name'], 'summary': summary})\n",
    "    return news\n",
    "\n",
    "base_directory = 'data'\n",
    "directories = {\n",
    "    'landmarks': 'landmarks',\n",
    "    'municipalities': 'municipalities',\n",
    "    'news': [\n",
    "        'elmundo_chunked_en_page1_15years',\n",
    "        'elmundo_chunked_es_page1_15years',\n",
    "        'elmundo_chunked_es_page1_40years'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Read and extract data from landmarks\n",
    "landmarks_data = read_files_in_directory(os.path.join(base_directory, directories['landmarks']))\n",
    "landmarks = extract_landmarks_data(landmarks_data)\n",
    "\n",
    "# Read and extract data from municipalities\n",
    "municipalities_data = read_files_in_directory(os.path.join(base_directory, directories['municipalities']))\n",
    "municipalities = extract_municipalities_data(municipalities_data)\n",
    "\n",
    "# Read and extract data from news\n",
    "news_data = []\n",
    "for news_dir in directories['news']:\n",
    "    news_data.extend(read_files_in_directory(os.path.join(base_directory, news_dir)))\n",
    "news = extract_news_data(news_data)\n",
    "\n",
    "# Combine all data into a single repository\n",
    "data_repository = {\n",
    "    'landmarks': landmarks,\n",
    "    'municipalities': municipalities,\n",
    "    'news': news\n",
    "}\n",
    "\n",
    "# Save the data repository to a JSON file\n",
    "with open('puerto_rico_data_repository.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(data_repository, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print('Data repository created successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# Load the GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "def summarize_text(text):\n",
    "    input_text = \"summarize: \" + text\n",
    "    inputs = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    \n",
    "    outputs = model.generate(inputs, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T5 Transformer : Hugging Face text summarization (NLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip uninstall sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade transformers\n",
    "!pip install --upgrade torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "\n",
    "# Load the T5 model and tokenizer\n",
    "model_name = \"t5-small\"\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "def read_files_in_directory(directory):\n",
    "    data = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if not file.endswith('.zip'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "                    data.append({'file_name': file, 'content': content})\n",
    "    return data\n",
    "\n",
    "def summarize_text(text):\n",
    "    # Preprocess the text for T5\n",
    "    input_text = \"summarize: \" + text\n",
    "    inputs = tokenizer.encode(input_text, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "    \n",
    "    # Generate the summary\n",
    "    summary_ids = model.generate(inputs, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return summary\n",
    "\n",
    "def extract_landmarks_data(data):\n",
    "    landmarks = []\n",
    "    for item in data:\n",
    "        summary = summarize_text(item['content'])\n",
    "        landmarks.append({'file_name': item['file_name'], 'summary': summary})\n",
    "    return landmarks\n",
    "\n",
    "def extract_municipalities_data(data):\n",
    "    municipalities = []\n",
    "    for item in data:\n",
    "        summary = summarize_text(item['content'])\n",
    "        municipalities.append({'file_name': item['file_name'], 'summary': summary})\n",
    "    return municipalities\n",
    "\n",
    "def extract_news_data(data):\n",
    "    news = []\n",
    "    for item in data:\n",
    "        summary = summarize_text(item['content'])\n",
    "        news.append({'file_name': item['file_name'], 'summary': summary})\n",
    "    return news\n",
    "\n",
    "base_directory = 'data'\n",
    "directories = {\n",
    "    'landmarks': 'landmarks',\n",
    "    'municipalities': 'municipalities',\n",
    "    'news': [\n",
    "        'elmundo_chunked_en_page1_15years',\n",
    "        'elmundo_chunked_es_page1_15years',\n",
    "        'elmundo_chunked_es_page1_40years'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Read and extract data from landmarks\n",
    "landmarks_data = read_files_in_directory(os.path.join(base_directory, directories['landmarks']))\n",
    "landmarks = extract_landmarks_data(landmarks_data)\n",
    "\n",
    "# Read and extract data from municipalities\n",
    "municipalities_data = read_files_in_directory(os.path.join(base_directory, directories['municipalities']))\n",
    "municipalities = extract_municipalities_data(municipalities_data)\n",
    "\n",
    "# Read and extract data from news\n",
    "news_data = []\n",
    "for news_dir in directories['news']:\n",
    "    news_data.extend(read_files_in_directory(os.path.join(base_directory, news_dir)))\n",
    "news = extract_news_data(news_data)\n",
    "\n",
    "# Combine all data into a single repository\n",
    "data_repository = {\n",
    "    'landmarks': landmarks,\n",
    "    'municipalities': municipalities,\n",
    "    'news': news\n",
    "}\n",
    "\n",
    "# Save the data repository to a JSON file\n",
    "with open('puerto_rico_data_repository.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(data_repository, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print('Data repository created successfully.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BART (Bidirectional and Auto-Regressive Transformers): A model designed for sequence-to-sequence tasks, including summarization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer\n",
    "\n",
    "# Load the BART model and tokenizer\n",
    "model_name = \"facebook/bart-large-cnn\"\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def read_files_in_directory(directory):\n",
    "    data = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if not file.endswith('.zip'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    content = f.read()\n",
    "                    data.append({'file_name': file, 'content': content})\n",
    "    return data\n",
    "\n",
    "def summarize_text(text):\n",
    "    # Preprocess the text for BART\n",
    "    inputs = tokenizer.encode(\"summarize: \" + text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "    \n",
    "    # Generate the summary\n",
    "    summary_ids = model.generate(inputs, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    return summary\n",
    "\n",
    "def extract_landmarks_data(data):\n",
    "    landmarks = []\n",
    "    for item in data:\n",
    "        summary = summarize_text(item['content'])\n",
    "        landmarks.append({'file_name': item['file_name'], 'summary': summary})\n",
    "    return landmarks\n",
    "\n",
    "def extract_municipalities_data(data):\n",
    "    municipalities = []\n",
    "    for item in data:\n",
    "        summary = summarize_text(item['content'])\n",
    "        municipalities.append({'file_name': item['file_name'], 'summary': summary})\n",
    "    return municipalities\n",
    "\n",
    "def extract_news_data(data):\n",
    "    news = []\n",
    "    for item in data:\n",
    "        summary = summarize_text(item['content'])\n",
    "        news.append({'file_name': item['file_name'], 'summary': summary})\n",
    "    return news\n",
    "\n",
    "base_directory = 'data'\n",
    "directories = {\n",
    "    'landmarks': 'landmarks',\n",
    "    'municipalities': 'municipalities',\n",
    "    'news': [\n",
    "        'elmundo_chunked_en_page1_15years',\n",
    "        'elmundo_chunked_es_page1_15years',\n",
    "        'elmundo_chunked_es_page1_40years'\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Read and extract data from landmarks\n",
    "landmarks_data = read_files_in_directory(os.path.join(base_directory, directories['landmarks']))\n",
    "landmarks = extract_landmarks_data(landmarks_data)\n",
    "\n",
    "# Read and extract data from municipalities\n",
    "municipalities_data = read_files_in_directory(os.path.join(base_directory, directories['municipalities']))\n",
    "municipalities = extract_municipalities_data(municipalities_data)\n",
    "\n",
    "# Read and extract data from news\n",
    "news_data = []\n",
    "for news_dir in directories['news']:\n",
    "    news_data.extend(read_files_in_directory(os.path.join(base_directory, news_dir)))\n",
    "news = extract_news_data(news_data)\n",
    "\n",
    "# Combine all data into a single repository\n",
    "data_repository = {\n",
    "    'landmarks': landmarks,\n",
    "    'municipalities': municipalities,\n",
    "    'news': news\n",
    "}\n",
    "\n",
    "# Save the data repository to a JSON file\n",
    "with open('puerto_rico_data_repository.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(data_repository, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "print('Data repository created successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Create chat input and output widgets\n",
    "chat_input = widgets.Text(description=\"User Input:\")\n",
    "chat_output = widgets.Output()\n",
    "\n",
    "# Function to handle user input\n",
    "def handle_submit(sender):\n",
    "    user_input = chat_input.value\n",
    "    with chat_output:\n",
    "        print(f\"User: {user_input}\")\n",
    "        # Add AI processing logic here\n",
    "        # Display AI response in the chat_output\n",
    "\n",
    "chat_input.on_submit(handle_submit)\n",
    "\n",
    "# Display chat UI\n",
    "display(chat_input, chat_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install Flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
